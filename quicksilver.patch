diff --git a/README b/README
index 8a504e0cc33..b7c62b1f4e9 100644
--- a/README
+++ b/README
@@ -1,19 +1,3 @@
-This repo is dedicated to use an async daemon to
-promote FreeBSD data pages as superpages.
-
-It starts from FreeBSD 11.2 release. There are
-3 things to be developed:
-	1. The async daemon should be able to scan
-	reservations in the part-pop queue periodically.
-
-	2. The early promotion code should be able
-	to fully-populate a reservation with non-temporal
-	stores
-
-	3. Upon promotion, the pv-entries to the 4KB pages must be discarded, i.e. the memory of which must be recycled.
-
-
-
 This is the top level of the FreeBSD source directory.  This file
 was last revised on:
 $FreeBSD$
diff --git a/lib/libc/amd64/string/Makefile.inc b/lib/libc/amd64/string/Makefile.inc
index 425de40d66b..db88ac72353 100644
--- a/lib/libc/amd64/string/Makefile.inc
+++ b/lib/libc/amd64/string/Makefile.inc
@@ -2,8 +2,6 @@
 
 MDSRCS+= \
 	bcmp.S \
-	bcopy.S \
-	bzero.S \
 	memcmp.S \
 	memcpy.S \
 	memmove.S \
diff --git a/lib/libc/amd64/string/bcmp.S b/lib/libc/amd64/string/bcmp.S
index d01b76bc10e..efdc6d33e4d 100644
--- a/lib/libc/amd64/string/bcmp.S
+++ b/lib/libc/amd64/string/bcmp.S
@@ -1,27 +1,121 @@
+/*-
+ * Copyright (c) 2018 The FreeBSD Foundation
+ *
+ * This software was developed by Mateusz Guzik <mjg@FreeBSD.org>
+ * under sponsorship from the FreeBSD Foundation.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR AND CONTRIBUTORS ``AS IS'' AND
+ * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED.  IN NO EVENT SHALL THE AUTHOR OR CONTRIBUTORS BE LIABLE
+ * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+ * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
+ * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
+ * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+ * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
+ * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
+ * SUCH DAMAGE.
+ *
+ * $FreeBSD$
+ */
+
 #include <machine/asm.h>
 __FBSDID("$FreeBSD$");
 
-#if 0
-	RCSID("$NetBSD: bcmp.S,v 1.1 2001/06/19 00:25:04 fvdl Exp $")
-#endif
-
 ENTRY(bcmp)
-	cld				/* set compare direction forward */
+	cmpq	$16,%rdx
+	jae	5f
+1:
+	testq	%rdx,%rdx
+	je	3f
+	xorl	%ecx,%ecx
+2:
+	movzbl	(%rdi,%rcx,1),%eax
+	movzbl	(%rsi,%rcx,1),%r8d
+	cmpb	%r8b,%al
+	jne	4f
+	addq	$1,%rcx
+	cmpq	%rcx,%rdx
+	jz	3f
+	movzbl	(%rdi,%rcx,1),%eax
+	movzbl	(%rsi,%rcx,1),%r8d
+	cmpb	%r8b,%al
+	jne	4f
+	addq	$1,%rcx
+	cmpq	%rcx,%rdx
+	jz	3f
+	movzbl	(%rdi,%rcx,1),%eax
+	movzbl	(%rsi,%rcx,1),%r8d
+	cmpb	%r8b,%al
+	jne	4f
+	addq	$1,%rcx
+	cmpq	%rcx,%rdx
+	jz	3f
+	movzbl	(%rdi,%rcx,1),%eax
+	movzbl	(%rsi,%rcx,1),%r8d
+	cmpb	%r8b,%al
+	jne	4f
+	addq	$1,%rcx
+	cmpq	%rcx,%rdx
+	jne	2b
+3:
+	xorl	%eax,%eax
+	ret
+4:
+	movl	$1,%eax
+	ret
+5:
+	cmpq	$32,%rdx
+	jae	7f
+6:
+	/*
+	 * 8 bytes
+	 */
+	movq	(%rdi),%r8
+	movq	(%rsi),%r9
+	cmpq	%r8,%r9
+	jne	4b
+	leaq	8(%rdi),%rdi
+	leaq	8(%rsi),%rsi
+	subq	$8,%rdx
+	cmpq	$8,%rdx
+	jae	6b
+	jl	1b
+	jmp	3b
+7:
+	/*
+	 * 32 bytes
+	 */
+	movq	(%rsi),%r8
+	movq	8(%rsi),%r9
+	subq	(%rdi),%r8
+	subq	8(%rdi),%r9
+	or	%r8,%r9
+	jnz	4b
 
-	movq	%rdx,%rcx		/* compare by words */
-	shrq	$3,%rcx
-	repe
-	cmpsq
-	jne	L1
+	movq	16(%rsi),%r8
+	movq	24(%rsi),%r9
+	subq	16(%rdi),%r8
+	subq	24(%rdi),%r9
+	or	%r8,%r9
+	jnz	4b
 
-	movq	%rdx,%rcx		/* compare remainder by bytes */
-	andq	$7,%rcx
-	repe
-	cmpsb
-L1:
-	setne	%al
-	movsbl	%al,%eax
-	ret
+	leaq	32(%rdi),%rdi
+	leaq	32(%rsi),%rsi
+	subq	$32,%rdx
+	cmpq	$32,%rdx
+	jae	7b
+	jnz	1b
+	jmp	3b
 END(bcmp)
 
 	.section .note.GNU-stack,"",%progbits
diff --git a/lib/libc/amd64/string/bcopy.S b/lib/libc/amd64/string/bcopy.S
deleted file mode 100644
index cc38f475525..00000000000
--- a/lib/libc/amd64/string/bcopy.S
+++ /dev/null
@@ -1,99 +0,0 @@
-/*-
- * Copyright (c) 1990 The Regents of the University of California.
- * All rights reserved.
- *
- * This code is derived from locore.s.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- * 1. Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- * 2. Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- * 3. Neither the name of the University nor the names of its contributors
- *    may be used to endorse or promote products derived from this software
- *    without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE REGENTS AND CONTRIBUTORS ``AS IS'' AND
- * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
- * ARE DISCLAIMED.  IN NO EVENT SHALL THE REGENTS OR CONTRIBUTORS BE LIABLE
- * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
- * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
- * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
- * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
- * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
- * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
- * SUCH DAMAGE.
- */
-
-#include <machine/asm.h>
-__FBSDID("$FreeBSD$");
-
-#if 0
-	RCSID("$NetBSD: bcopy.S,v 1.2 2003/08/07 16:42:36 agc Exp $")
-#endif
-
-	/*
-	 * (ov)bcopy (src,dst,cnt)
-	 *  ws@tools.de     (Wolfgang Solfrank, TooLs GmbH) +49-228-985800
-	 */
-
-#ifdef MEMCOPY
-ENTRY(memcpy)
-#else
-#ifdef MEMMOVE
-ENTRY(memmove)
-#else
-ENTRY(bcopy)
-#endif
-#endif
-#if defined(MEMCOPY) || defined(MEMMOVE)
-	movq	%rdi,%rax	/* return dst */
-#else
-	xchgq	%rdi,%rsi
-#endif
-	movq	%rdx,%rcx
-	movq	%rdi,%r8
-	subq	%rsi,%r8
-	cmpq	%rcx,%r8	/* overlapping? */
-	jb	1f
-	cld			/* nope, copy forwards. */
-	shrq	$3,%rcx		/* copy by words */
-	rep
-	movsq
-	movq	%rdx,%rcx
-	andq	$7,%rcx		/* any bytes left? */
-	rep
-	movsb
-	ret
-1:
-	addq	%rcx,%rdi	/* copy backwards. */
-	addq	%rcx,%rsi
-	std
-	andq	$7,%rcx		/* any fractional bytes? */
-	decq	%rdi
-	decq	%rsi
-	rep
-	movsb
-	movq	%rdx,%rcx	/* copy remainder by words */
-	shrq	$3,%rcx
-	subq	$7,%rsi
-	subq	$7,%rdi
-	rep
-	movsq
-	cld
-	ret
-#ifdef MEMCOPY
-END(memcpy)
-#else
-#ifdef MEMMOVE
-END(memmove)
-#else
-END(bcopy)
-#endif
-#endif
-
-	.section .note.GNU-stack,"",%progbits
diff --git a/lib/libc/amd64/string/bcopy.c b/lib/libc/amd64/string/bcopy.c
new file mode 100644
index 00000000000..9e0c4187e43
--- /dev/null
+++ b/lib/libc/amd64/string/bcopy.c
@@ -0,0 +1,15 @@
+/*-
+ * Public domain.
+ */
+
+#include <sys/cdefs.h>
+__FBSDID("$FreeBSD$");
+
+#include <string.h>
+
+void
+bcopy(const void *src, void *dst, size_t len)
+{
+
+	memmove(dst, src, len);
+}
diff --git a/lib/libc/amd64/string/bzero.S b/lib/libc/amd64/string/bzero.S
deleted file mode 100644
index cf46a2a317b..00000000000
--- a/lib/libc/amd64/string/bzero.S
+++ /dev/null
@@ -1,46 +0,0 @@
-/*
- * Written by J.T. Conklin <jtc@NetBSD.org>.
- * Public domain.
- * Adapted for NetBSD/x86_64 by Frank van der Linden <fvdl@wasabisystems.com>
- */
-
-#include <machine/asm.h>
-__FBSDID("$FreeBSD$");
-
-#if 0
-	RCSID("$NetBSD: bzero.S,v 1.2 2003/07/26 19:24:38 salo Exp $")
-#endif
-
-ENTRY(bzero)
-	cld				/* set fill direction forward */
-	xorq	%rax,%rax		/* set fill data to 0 */
-
-	/*
-	 * if the string is too short, it's really not worth the overhead
-	 * of aligning to word boundries, etc.  So we jump to a plain
-	 * unaligned set.
-	 */
-	cmpq	$16,%rsi
-	jb	L1
-
-	movq	%rdi,%rcx		/* compute misalignment */
-	negq	%rcx
-	andq	$7,%rcx
-	subq	%rcx,%rsi
-	rep				/* zero until word aligned */
-	stosb
-
-	movq	%rsi,%rcx		/* zero by words */
-	shrq	$3,%rcx
-	andq	$7,%rsi
-	rep
-	stosq
-
-L1:	movq	%rsi,%rcx		/* zero remainder by bytes */
-	rep
-	stosb
-
-	ret
-END(bzero)
-
-	.section .note.GNU-stack,"",%progbits
diff --git a/lib/libc/amd64/string/bzero.c b/lib/libc/amd64/string/bzero.c
new file mode 100644
index 00000000000..1ab391076b0
--- /dev/null
+++ b/lib/libc/amd64/string/bzero.c
@@ -0,0 +1,15 @@
+/*-
+ * Public domain.
+ */
+
+#include <sys/cdefs.h>
+__FBSDID("$FreeBSD$");
+
+#include <string.h>
+
+void
+bzero(void *b, size_t len)
+{
+
+	memset(b, 0, len);
+}
diff --git a/lib/libc/amd64/string/memcmp.S b/lib/libc/amd64/string/memcmp.S
index 66d64a0b5a4..3012ffa688c 100644
--- a/lib/libc/amd64/string/memcmp.S
+++ b/lib/libc/amd64/string/memcmp.S
@@ -1,44 +1,121 @@
-/*
- * Written by J.T. Conklin <jtc@NetBSD.org>.
- * Public domain.
- * Adapted for NetBSD/x86_64 by Frank van der Linden <fvdl@wasabisystems.com>
+/*-
+ * Copyright (c) 2018 The FreeBSD Foundation
+ *
+ * This software was developed by Mateusz Guzik <mjg@FreeBSD.org>
+ * under sponsorship from the FreeBSD Foundation.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR AND CONTRIBUTORS ``AS IS'' AND
+ * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED.  IN NO EVENT SHALL THE AUTHOR OR CONTRIBUTORS BE LIABLE
+ * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+ * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
+ * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
+ * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+ * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
+ * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
+ * SUCH DAMAGE.
+ *
+ * $FreeBSD$
  */
 
 #include <machine/asm.h>
 __FBSDID("$FreeBSD$");
 
-#if 0
-	RCSID("$NetBSD: memcmp.S,v 1.2 2003/07/26 19:24:39 salo Exp $")
-#endif
-
 ENTRY(memcmp)
-	cld				/* set compare direction forward */
-	movq	%rdx,%rcx		/* compare by longs */
-	shrq	$3,%rcx
-	repe
-	cmpsq
-	jne	L5			/* do we match so far? */
-
-	movq	%rdx,%rcx		/* compare remainder by bytes */
-	andq	$7,%rcx
-	repe
-	cmpsb
-	jne	L6			/* do we match? */
-
-	xorl	%eax,%eax		/* we match, return zero	*/
+	cmpq	$16,%rdx
+	jae	5f
+1:
+	testq	%rdx,%rdx
+	je	3f
+	xorl	%ecx,%ecx
+2:
+	movzbl	(%rdi,%rcx,1),%eax
+	movzbl	(%rsi,%rcx,1),%r8d
+	cmpb	%r8b,%al
+	jne	4f
+	addq	$1,%rcx
+	cmpq	%rcx,%rdx
+	jz	3f
+	movzbl	(%rdi,%rcx,1),%eax
+	movzbl	(%rsi,%rcx,1),%r8d
+	cmpb	%r8b,%al
+	jne	4f
+	addq	$1,%rcx
+	cmpq	%rcx,%rdx
+	jz	3f
+	movzbl	(%rdi,%rcx,1),%eax
+	movzbl	(%rsi,%rcx,1),%r8d
+	cmpb	%r8b,%al
+	jne	4f
+	addq	$1,%rcx
+	cmpq	%rcx,%rdx
+	jz	3f
+	movzbl	(%rdi,%rcx,1),%eax
+	movzbl	(%rsi,%rcx,1),%r8d
+	cmpb	%r8b,%al
+	jne	4f
+	addq	$1,%rcx
+	cmpq	%rcx,%rdx
+	jne	2b
+3:
+	xorl	%eax,%eax
 	ret
-
-L5:	movl	$8,%ecx			/* We know that one of the next	*/
-	subq	%rcx,%rdi		/* eight pairs of bytes do not	*/
-	subq	%rcx,%rsi		/* match.			*/
-	repe
-	cmpsb
-L6:	xorl	%eax,%eax		/* Perform unsigned comparison	*/
-	movb	-1(%rdi),%al
-	xorl	%edx,%edx
-	movb	-1(%rsi),%dl
-	subl    %edx,%eax
+4:
+	subl	%r8d,%eax
 	ret
+5:
+	cmpq	$32,%rdx
+	jae	7f
+6:
+	/*
+	 * 8 bytes
+	 */
+	movq	(%rdi),%r8
+	movq	(%rsi),%r9
+	cmpq	%r8,%r9
+	jne	1b
+	leaq	8(%rdi),%rdi
+	leaq	8(%rsi),%rsi
+	subq	$8,%rdx
+	cmpq	$8,%rdx
+	jae	6b
+	jl	1b
+	jmp	3b
+7:
+	/*
+	 * 32 bytes
+	 */
+	movq	(%rsi),%r8
+	movq	8(%rsi),%r9
+	subq	(%rdi),%r8
+	subq	8(%rdi),%r9
+	or	%r8,%r9
+	jnz	1b
+
+	movq	16(%rsi),%r8
+	movq	24(%rsi),%r9
+	subq	16(%rdi),%r8
+	subq	24(%rdi),%r9
+	or	%r8,%r9
+	jnz	1b
+
+	leaq	32(%rdi),%rdi
+	leaq	32(%rsi),%rsi
+	subq	$32,%rdx
+	cmpq	$32,%rdx
+	jae	7b
+	jnz	1b
+	jmp	3b
 END(memcmp)
 
 	.section .note.GNU-stack,"",%progbits
diff --git a/lib/libc/amd64/string/memcpy.S b/lib/libc/amd64/string/memcpy.S
index bd1e8422ad8..2b6c73abeb9 100644
--- a/lib/libc/amd64/string/memcpy.S
+++ b/lib/libc/amd64/string/memcpy.S
@@ -1,5 +1,5 @@
 /*	$NetBSD: memcpy.S,v 1.1 2001/06/19 00:25:05 fvdl Exp $	*/
 /*	$FreeBSD$ */
 
-#define MEMCOPY
-#include "bcopy.S"
+#define MEMCPY
+#include "memmove.S"
diff --git a/lib/libc/amd64/string/memmove.S b/lib/libc/amd64/string/memmove.S
index 85beb262f8d..9d6fa8a7e29 100644
--- a/lib/libc/amd64/string/memmove.S
+++ b/lib/libc/amd64/string/memmove.S
@@ -1,5 +1,270 @@
-/*	$NetBSD: memmove.S,v 1.1 2001/06/19 00:25:05 fvdl Exp $	*/
-/*	$FreeBSD$ */
+/*-
+ * Copyright (c) 2018 The FreeBSD Foundation
+ *
+ * This software was developed by Mateusz Guzik <mjg@FreeBSD.org>
+ * under sponsorship from the FreeBSD Foundation.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR AND CONTRIBUTORS ``AS IS'' AND
+ * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED.  IN NO EVENT SHALL THE AUTHOR OR CONTRIBUTORS BE LIABLE
+ * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+ * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
+ * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
+ * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+ * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
+ * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
+ * SUCH DAMAGE.
+ */
 
-#define MEMMOVE
-#include "bcopy.S"
+#include <machine/asm.h>
+__FBSDID("$FreeBSD$");
+
+#define	ALIGN_TEXT	.p2align 4,0x90 /* 16-byte alignment, nop filled */
+
+/*
+ * memmove(dst, src, cnt)
+ *         rdi, rsi, rdx
+ * Contains parts of bcopy written by:
+ *  ws@tools.de     (Wolfgang Solfrank, TooLs GmbH) +49-228-985800
+ */
+
+/*
+ * Register state at entry is supposed to be as follows:
+ * rdi - destination
+ * rsi - source
+ * rdx - count
+ *
+ * The macro possibly clobbers the above and: rcx, r8.
+ * It does not clobber rax, r10 nor r11.
+ */
+.macro MEMMOVE erms overlap begin end
+	\begin
+.if \overlap == 1
+	movq	%rdi,%r8
+	subq	%rsi,%r8
+	cmpq	%rcx,%r8	/* overlapping && src < dst? */
+	jb	2f
+.endif
+
+	cmpq	$32,%rcx
+	jb	1016f
+
+	cmpq	$256,%rcx
+	ja	1256f
+
+1032:
+	movq	(%rsi),%rdx
+	movq	%rdx,(%rdi)
+	movq	8(%rsi),%rdx
+	movq	%rdx,8(%rdi)
+	movq	16(%rsi),%rdx
+	movq	%rdx,16(%rdi)
+	movq	24(%rsi),%rdx
+	movq	%rdx,24(%rdi)
+	leaq	32(%rsi),%rsi
+	leaq	32(%rdi),%rdi
+	subq	$32,%rcx
+	cmpq	$32,%rcx
+	jae	1032b
+	cmpb	$0,%cl
+	jne	1016f
+	\end
+	ret
+	ALIGN_TEXT
+1016:
+	cmpb	$16,%cl
+	jl	1008f
+	movq	(%rsi),%rdx
+	movq	%rdx,(%rdi)
+	movq	8(%rsi),%rdx
+	movq	%rdx,8(%rdi)
+	subb	$16,%cl
+	jz	1000f
+	leaq	16(%rsi),%rsi
+	leaq	16(%rdi),%rdi
+1008:
+	cmpb	$8,%cl
+	jl	1004f
+	movq	(%rsi),%rdx
+	movq	%rdx,(%rdi)
+	subb	$8,%cl
+	jz	1000f
+	leaq	8(%rsi),%rsi
+	leaq	8(%rdi),%rdi
+1004:
+	cmpb	$4,%cl
+	jl	1002f
+	movl	(%rsi),%edx
+	movl	%edx,(%rdi)
+	subb	$4,%cl
+	jz	1000f
+	leaq	4(%rsi),%rsi
+	leaq	4(%rdi),%rdi
+1002:
+	cmpb	$2,%cl
+	jl	1001f
+	movw	(%rsi),%dx
+	movw	%dx,(%rdi)
+	subb	$2,%cl
+	jz	1000f
+	leaq	2(%rsi),%rsi
+	leaq	2(%rdi),%rdi
+1001:
+	cmpb	$1,%cl
+	jl	1000f
+	movb	(%rsi),%dl
+	movb	%dl,(%rdi)
+1000:
+	\end
+	ret
+
+	ALIGN_TEXT
+1256:
+.if \erms == 1
+	rep
+	movsb
+.else
+	shrq	$3,%rcx                         /* copy by 64-bit words */
+	rep
+	movsq
+	movq	%rdx,%rcx
+	andb	$7,%cl                         /* any bytes left? */
+	jne	1004b
+.endif
+	\end
+	ret
+
+.if \overlap == 1
+	/*
+	 * Copy backwards.
+	 */
+        ALIGN_TEXT
+2:
+	addq	%rcx,%rdi
+	addq	%rcx,%rsi
+
+	cmpq	$32,%rcx
+	jb	2016f
+
+	cmpq	$256,%rcx
+	ja	2256f
+
+2032:
+	movq	-8(%rsi),%rdx
+	movq	%rdx,-8(%rdi)
+	movq	-16(%rsi),%rdx
+	movq	%rdx,-16(%rdi)
+	movq	-24(%rsi),%rdx
+	movq	%rdx,-24(%rdi)
+	movq	-32(%rsi),%rdx
+	movq	%rdx,-32(%rdi)
+	leaq	-32(%rsi),%rsi
+	leaq	-32(%rdi),%rdi
+	subq	$32,%rcx
+	cmpq	$32,%rcx
+	jae	2032b
+	cmpb	$0,%cl
+	jne	2016f
+	\end
+	ret
+	ALIGN_TEXT
+2016:
+	cmpb	$16,%cl
+	jl	2008f
+	movq	-8(%rsi),%rdx
+	movq	%rdx,-8(%rdi)
+	movq	-16(%rsi),%rdx
+	movq	%rdx,-16(%rdi)
+	subb	$16,%cl
+	jz	2000f
+	leaq	-16(%rsi),%rsi
+	leaq	-16(%rdi),%rdi
+2008:
+	cmpb	$8,%cl
+	jl	2004f
+	movq	-8(%rsi),%rdx
+	movq	%rdx,-8(%rdi)
+	subb	$8,%cl
+	jz	2000f
+	leaq	-8(%rsi),%rsi
+	leaq	-8(%rdi),%rdi
+2004:
+	cmpb	$4,%cl
+	jl	2002f
+	movl	-4(%rsi),%edx
+	movl	%edx,-4(%rdi)
+	subb	$4,%cl
+	jz	2000f
+	leaq	-4(%rsi),%rsi
+	leaq	-4(%rdi),%rdi
+2002:
+	cmpb	$2,%cl
+	jl	2001f
+	movw	-2(%rsi),%dx
+	movw	%dx,-2(%rdi)
+	subb	$2,%cl
+	jz	2000f
+	leaq	-2(%rsi),%rsi
+	leaq	-2(%rdi),%rdi
+2001:
+	cmpb	$1,%cl
+	jl	2000f
+	movb	-1(%rsi),%dl
+	movb	%dl,-1(%rdi)
+2000:
+	\end
+	ret
+	ALIGN_TEXT
+2256:
+	decq	%rdi
+	decq	%rsi
+	std
+.if \erms == 1
+	rep
+	movsb
+.else
+	andq	$7,%rcx                         /* any fractional bytes? */
+	je	3f
+	rep
+	movsb
+3:
+	movq	%rdx,%rcx                       /* copy remainder by 32-bit words */
+	shrq	$3,%rcx
+	subq	$7,%rsi
+	subq	$7,%rdi
+	rep
+	movsq
+.endif
+	cld
+	\end
+	ret
+.endif
+.endm
+
+.macro MEMMOVE_BEGIN
+	movq	%rdi,%rax
+	movq	%rdx,%rcx
+.endm
+
+.macro MEMMOVE_END
+.endm
+
+#ifndef MEMCPY
+ENTRY(memmove)
+	MEMMOVE erms=0 overlap=1 begin=MEMMOVE_BEGIN end=MEMMOVE_END
+END(memmove)
+#else
+ENTRY(memcpy)
+	MEMMOVE erms=0 overlap=1 begin=MEMMOVE_BEGIN end=MEMMOVE_END
+END(memcpy)
+#endif
diff --git a/lib/libc/amd64/string/memset.S b/lib/libc/amd64/string/memset.S
index 84d15628766..67f21714b3d 100644
--- a/lib/libc/amd64/string/memset.S
+++ b/lib/libc/amd64/string/memset.S
@@ -1,63 +1,142 @@
-/*
- * Written by J.T. Conklin <jtc@NetBSD.org>.
- * Public domain.
- * Adapted for NetBSD/x86_64 by Frank van der Linden <fvdl@wasabisystems.com>
+/*-
+ * Copyright (c) 2018 The FreeBSD Foundation
+ *
+ * This software was developed by Mateusz Guzik <mjg@FreeBSD.org>
+ * under sponsorship from the FreeBSD Foundation.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR AND CONTRIBUTORS ``AS IS'' AND
+ * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED.  IN NO EVENT SHALL THE AUTHOR OR CONTRIBUTORS BE LIABLE
+ * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+ * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
+ * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
+ * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+ * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
+ * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
+ * SUCH DAMAGE.
+ *
+ * $FreeBSD$
  */
 
 #include <machine/asm.h>
 __FBSDID("$FreeBSD$");
 
-#if 0
-	RCSID("$NetBSD: memset.S,v 1.3 2004/02/26 20:50:06 drochner Exp $")
-#endif
+#define ALIGN_TEXT      .p2align 4,0x90 /* 16-byte alignment, nop filled */
 
-ENTRY(memset)
-	movq	%rsi,%rax
-	andq	$0xff,%rax
+.macro MEMSET erms
+	movq	%rdi,%rax
 	movq	%rdx,%rcx
-	movq	%rdi,%r11
-
-	cld				/* set fill direction forward */
-
-	/*
-	 * if the string is too short, it's really not worth the overhead
-	 * of aligning to word boundries, etc.  So we jump to a plain
-	 * unaligned set.
-	 */
-	cmpq	$0x0f,%rcx
-	jle	L1
-
-	movb	%al,%ah			/* copy char to all bytes in word */
-	movl	%eax,%edx
-	sall	$16,%eax
-	orl	%edx,%eax
-
-	movl	%eax,%edx
-	salq	$32,%rax
-	orq	%rdx,%rax
-
-	movq	%rdi,%rdx		/* compute misalignment */
-	negq	%rdx
-	andq	$7,%rdx
-	movq	%rcx,%r8
-	subq	%rdx,%r8
-
-	movq	%rdx,%rcx		/* set until word aligned */
+	movzbq	%sil,%r8
+	movabs	$0x0101010101010101,%r10
+	imulq	%r8,%r10
+
+	cmpq	$32,%rcx
+	jbe	101632f
+
+	cmpq	$256,%rcx
+	ja	1256f
+
+103200:
+	movq	%r10,(%rdi)
+	movq	%r10,8(%rdi)
+	movq	%r10,16(%rdi)
+	movq	%r10,24(%rdi)
+	leaq	32(%rdi),%rdi
+	subq	$32,%rcx
+	cmpq	$32,%rcx
+	ja	103200b
+	cmpb	$16,%cl
+	ja	201632f
+	movq	%r10,-16(%rdi,%rcx)
+	movq	%r10,-8(%rdi,%rcx)
+	ret
+	ALIGN_TEXT
+101632:
+	cmpb	$16,%cl
+	jl	100816f
+201632:
+	movq	%r10,(%rdi)
+	movq	%r10,8(%rdi)
+	movq	%r10,-16(%rdi,%rcx)
+	movq	%r10,-8(%rdi,%rcx)
+	ret
+	ALIGN_TEXT
+100816:
+	cmpb	$8,%cl
+	jl	100408f
+	movq	%r10,(%rdi)
+	movq	%r10,-8(%rdi,%rcx)
+	ret
+	ALIGN_TEXT
+100408:
+	cmpb	$4,%cl
+	jl	100204f
+	movl	%r10d,(%rdi)
+	movl	%r10d,-4(%rdi,%rcx)
+	ret
+	ALIGN_TEXT
+100204:
+	cmpb	$2,%cl
+	jl	100001f
+	movw	%r10w,(%rdi)
+	movw	%r10w,-2(%rdi,%rcx)
+	ret
+	ALIGN_TEXT
+100001:
+	cmpb	$0,%cl
+	je	100000f
+	movb	%r10b,(%rdi)
+100000:
+	ret
+	ALIGN_TEXT
+1256:
+	movq	%rdi,%r9
+	movq	%r10,%rax
+	testl	$15,%edi
+	jnz	3f
+1:
+.if \erms == 1
 	rep
 	stosb
-
-	movq	%r8,%rcx
-	shrq	$3,%rcx			/* set by words */
+	movq	%r9,%rax
+.else
+	movq	%rcx,%rdx
+	shrq	$3,%rcx
 	rep
 	stosq
+	movq	%r9,%rax
+	andl	$7,%edx
+	jnz	2f
+	ret
+2:
+	movq	%r10,-8(%rdi,%rdx)
+.endif
+	ret
+	ALIGN_TEXT
+3:
+	movq	%r10,(%rdi)
+	movq	%r10,8(%rdi)
+	movq	%rdi,%r8
+	andq	$15,%r8
+	leaq	-16(%rcx,%r8),%rcx
+	neg	%r8
+	leaq	16(%rdi,%r8),%rdi
+	jmp	1b
+.endm
 
-	movq	%r8,%rcx		/* set remainder by bytes */
-	andq	$7,%rcx
-L1:	rep
-	stosb
-	movq	%r11,%rax
 
-	ret
+ENTRY(memset)
+	MEMSET erms=0
 END(memset)
 
 	.section .note.GNU-stack,"",%progbits
diff --git a/lib/libc/amd64/string/strcpy.c b/lib/libc/amd64/string/strcpy.c
index 11a24eb2cf3..df1facefb10 100644
--- a/lib/libc/amd64/string/strcpy.c
+++ b/lib/libc/amd64/string/strcpy.c
@@ -1,4 +1,6 @@
-/*
+/*-
+ * SPDX-License-Identifier: BSD-2-Clause-FreeBSD
+ *
  * Copyright 2011 George V. Neville-Neil. All rights reserved.
  *
  * The compilation of software known as FreeBSD is distributed under the
diff --git a/sys/amd64/amd64/pmap.c b/sys/amd64/amd64/pmap.c
index a837c4d2c85..ecfd6f55e9f 100644
--- a/sys/amd64/amd64/pmap.c
+++ b/sys/amd64/amd64/pmap.c
@@ -361,6 +361,12 @@ static int pg_ps_enabled = 1;
 SYSCTL_INT(_vm_pmap, OID_AUTO, pg_ps_enabled, CTLFLAG_RDTUN | CTLFLAG_NOFETCH,
     &pg_ps_enabled, 0, "Are large page mappings enabled?");
 
+// static int pmap_zero_cnt = 0, pmap_zero_idle_cnt = 0;
+// SYSCTL_INT(_vm_pmap, OID_AUTO, pmap_zero_cnt, CTLFLAG_RD,
+//     &pmap_zero_cnt, 0, "cnt of pmap_zero");
+// SYSCTL_INT(_vm_pmap, OID_AUTO, pmap_zero_idle_cnt, CTLFLAG_RD,
+//     &pmap_zero_idle_cnt, 0, "cnt of pmap_zero_idle");
+
 #define	PAT_INDEX_SIZE	8
 static int pat_index[PAT_INDEX_SIZE];	/* cache mode to PAT index conversion */
 
@@ -651,7 +657,7 @@ static void pmap_invalidate_pde_page(pmap_t pmap, vm_offset_t va,
 static void pmap_kenter_attr(vm_offset_t va, vm_paddr_t pa, int mode);
 static void pmap_pde_attr(pd_entry_t *pde, int cache_bits, int mask);
 #if VM_NRESERVLEVEL > 0
-static void pmap_promote_pde(pmap_t pmap, pd_entry_t *pde, vm_offset_t va,
+static int pmap_promote_pde(pmap_t pmap, pd_entry_t *pde, vm_offset_t va,
     struct rwlock **lockp);
 #endif
 static boolean_t pmap_protect_pde(pmap_t pmap, pd_entry_t *pde, vm_offset_t sva,
@@ -1348,6 +1354,22 @@ static u_long pmap_pde_p_failures;
 SYSCTL_ULONG(_vm_pmap_pde, OID_AUTO, p_failures, CTLFLAG_RD,
     &pmap_pde_p_failures, 0, "2MB page promotion failures");
 
+static u_long pmap_pde_p_failures_1;
+SYSCTL_ULONG(_vm_pmap_pde, OID_AUTO, p_failures_1, CTLFLAG_RD,
+    &pmap_pde_p_failures_1, 0, "2MB page promotion failures");
+
+static u_long pmap_pde_p_failures_2;
+SYSCTL_ULONG(_vm_pmap_pde, OID_AUTO, p_failures_2, CTLFLAG_RD,
+    &pmap_pde_p_failures_2, 0, "2MB page promotion failures");
+
+static u_long pmap_pde_p_failures_3;
+SYSCTL_ULONG(_vm_pmap_pde, OID_AUTO, p_failures_3, CTLFLAG_RD,
+    &pmap_pde_p_failures_3, 0, "2MB page promotion failures");
+
+static u_long pmap_pde_p_failures_4;
+SYSCTL_ULONG(_vm_pmap_pde, OID_AUTO, p_failures_4, CTLFLAG_RD,
+    &pmap_pde_p_failures_4, 0, "2MB page promotion failures");
+
 static u_long pmap_pde_promotions;
 SYSCTL_ULONG(_vm_pmap_pde, OID_AUTO, promotions, CTLFLAG_RD,
     &pmap_pde_promotions, 0, "2MB page promotions");
@@ -4519,17 +4541,17 @@ pmap_protect(pmap_t pmap, vm_offset_t sva, vm_offset_t eva, vm_prot_t prot)
  * aligned, contiguous physical memory and (2) the 4KB page mappings must have
  * identical characteristics. 
  */
-static void
+static int
 pmap_promote_pde(pmap_t pmap, pd_entry_t *pde, vm_offset_t va,
     struct rwlock **lockp)
 {
 	pd_entry_t newpde;
 	pt_entry_t *firstpte, oldpte, pa, *pte;
-	pt_entry_t PG_G, PG_A, PG_M, PG_RW, PG_V;
+	pt_entry_t PG_G, PG_M, PG_RW, PG_V;
 	vm_page_t mpte;
 	int PG_PTE_CACHE;
 
-	PG_A = pmap_accessed_bit(pmap);
+	// PG_A = pmap_accessed_bit(pmap);
 	PG_G = pmap_global_bit(pmap);
 	PG_M = pmap_modified_bit(pmap);
 	PG_V = pmap_valid_bit(pmap);
@@ -4546,11 +4568,12 @@ pmap_promote_pde(pmap_t pmap, pd_entry_t *pde, vm_offset_t va,
 	firstpte = (pt_entry_t *)PHYS_TO_DMAP(*pde & PG_FRAME);
 setpde:
 	newpde = *firstpte;
-	if ((newpde & ((PG_FRAME & PDRMASK) | PG_A | PG_V)) != (PG_A | PG_V)) {
+	if ((newpde & ((PG_FRAME & PDRMASK) | PG_V)) != PG_V) {
 		atomic_add_long(&pmap_pde_p_failures, 1);
+		atomic_add_long(&pmap_pde_p_failures_1, 1);
 		CTR2(KTR_PMAP, "pmap_promote_pde: failure for va %#lx"
 		    " in pmap %p", va, pmap);
-		return;
+		return -1;
 	}
 	if ((newpde & (PG_M | PG_RW)) == PG_RW) {
 		/*
@@ -4567,15 +4590,16 @@ pmap_promote_pde(pmap_t pmap, pd_entry_t *pde, vm_offset_t va,
 	 * PTE maps an unexpected 4KB physical page or does not have identical
 	 * characteristics to the first PTE.
 	 */
-	pa = (newpde & (PG_PS_FRAME | PG_A | PG_V)) + NBPDR - PAGE_SIZE;
+	pa = (newpde & (PG_PS_FRAME | PG_V)) + NBPDR - PAGE_SIZE;
 	for (pte = firstpte + NPTEPG - 1; pte > firstpte; pte--) {
 setpte:
 		oldpte = *pte;
-		if ((oldpte & (PG_FRAME | PG_A | PG_V)) != pa) {
+		if ((oldpte & (PG_FRAME | PG_V)) != pa) {
 			atomic_add_long(&pmap_pde_p_failures, 1);
+			atomic_add_long(&pmap_pde_p_failures_2, 1);
 			CTR2(KTR_PMAP, "pmap_promote_pde: failure for va %#lx"
 			    " in pmap %p", va, pmap);
-			return;
+			return -1;
 		}
 		if ((oldpte & (PG_M | PG_RW)) == PG_RW) {
 			/*
@@ -4591,9 +4615,11 @@ pmap_promote_pde(pmap_t pmap, pd_entry_t *pde, vm_offset_t va,
 		}
 		if ((oldpte & PG_PTE_PROMOTE) != (newpde & PG_PTE_PROMOTE)) {
 			atomic_add_long(&pmap_pde_p_failures, 1);
+			atomic_add_long(&pmap_pde_p_failures_3, 1);
 			CTR2(KTR_PMAP, "pmap_promote_pde: failure for va %#lx"
 			    " in pmap %p", va, pmap);
-			return;
+			// printf("fail 3: old|%lx new|%lx\n", oldpte, newpde);
+			return -1;
 		}
 		pa -= PAGE_SIZE;
 	}
@@ -4611,10 +4637,11 @@ pmap_promote_pde(pmap_t pmap, pd_entry_t *pde, vm_offset_t va,
 	    ("pmap_promote_pde: page table page's pindex is wrong"));
 	if (pmap_insert_pt_page(pmap, mpte)) {
 		atomic_add_long(&pmap_pde_p_failures, 1);
+		atomic_add_long(&pmap_pde_p_failures_4, 1);
 		CTR2(KTR_PMAP,
 		    "pmap_promote_pde: failure for va %#lx in pmap %p", va,
 		    pmap);
-		return;
+		return -1;
 	}
 
 	/*
@@ -4639,6 +4666,8 @@ pmap_promote_pde(pmap_t pmap, pd_entry_t *pde, vm_offset_t va,
 	atomic_add_long(&pmap_pde_promotions, 1);
 	CTR2(KTR_PMAP, "pmap_promote_pde: success for va %#lx"
 	    " in pmap %p", va, pmap);
+
+	return 0;
 }
 #endif /* VM_NRESERVLEVEL > 0 */
 
@@ -5115,6 +5144,7 @@ pmap_enter_quick_locked(pmap_t pmap, vm_offset_t va, vm_page_t m,
 	struct spglist free;
 	pt_entry_t *pte, PG_V;
 	vm_paddr_t pa;
+	int rv;
 
 	KASSERT(va < kmi.clean_sva || va >= kmi.clean_eva ||
 	    (m->oflags & VPO_UNMANAGED) != 0,
@@ -5215,6 +5245,23 @@ pmap_enter_quick_locked(pmap_t pmap, vm_offset_t va, vm_page_t m,
 		pte_store(pte, pa | PG_V | PG_U);
 	else
 		pte_store(pte, pa | PG_V | PG_U | PG_MANAGED);
+
+
+
+#if VM_NRESERVLEVEL > 0
+	/*
+	 * If both the page table page and the reservation are fully
+	 * populated, then attempt promotion.
+	 */
+	if ((mpte == NULL || mpte->wire_count == NPTEPG) &&
+	    pmap_ps_enabled(pmap) &&
+	    (m->flags & PG_FICTITIOUS) == 0 &&
+	    vm_reserv_level_iffullpop(m) == 0) {
+		rv = pmap_promote_pde(pmap, pmap_pde(pmap, va), va, lockp);
+		// printf("va: %lx, rv: %d, type: %d\n", va, rv, m->object->type);
+	}
+#endif
+
 	return (mpte);
 }
 
@@ -5596,6 +5643,7 @@ void
 pmap_zero_page(vm_page_t m)
 {
 	vm_offset_t va = PHYS_TO_DMAP(VM_PAGE_TO_PHYS(m));
+	// pmap_zero_cnt ++;
 
 	pagezero((void *)va);
 }
@@ -5622,13 +5670,44 @@ pmap_zero_page_area(vm_page_t m, int off, int size)
  *	the page into KVM and using bzero to clear its contents.  This
  *	is intended to be called from the vm_pagezero process only and
  *	outside of Giant.
+ *  [sync_promo]:
+ *		modified to zero page with no cache effect
  */
 void
 pmap_zero_page_idle(vm_page_t m)
 {
 	vm_offset_t va = PHYS_TO_DMAP(VM_PAGE_TO_PHYS(m));
+	// pmap_zero_idle_cnt ++;
+	sse2_pagezero((void *)va);
+}
 
-	pagezero((void *)va);
+/*
+ *	pmap_zero_page_area_idle zeros the specified hardware page by mapping 
+ *	the page into KVM and using sse2_pagezero to clear its contents.
+ *
+ *	off and size may not cover an area beyond a single hardware page.
+ */
+void
+pmap_zero_pages_idle(vm_page_t m, int npages)
+{
+	vm_offset_t va = PHYS_TO_DMAP(VM_PAGE_TO_PHYS(m));
+	/* 
+		[fast page zeroing]
+		npages is supposed to be <= 512 
+		one should always use this code to zero chunks of data
+		on graphchi(pagerank_3):
+			sse2_pagezero_chunk:
+				79.02 real       235.97 user        26.04 sys
+			sse2_pagezero:
+				80.04 real       240.45 user        27.29 sys
+	*/
+	sse2_pagezero_chunk((void *)va, npages << 12);
+	// while(npages > 0)
+	// {
+	// 	sse2_pagezero((void *)va);
+	// 	va += PAGE_SIZE;
+	// 	npages --;
+	// }
 }
 
 /*
@@ -6577,14 +6656,28 @@ pmap_advise(pmap_t pmap, vm_offset_t sva, vm_offset_t eva, int advice)
 			 * table page is fully populated, this removal never
 			 * frees a page table page.
 			 */
-			if ((oldpde & PG_W) == 0) {
-				pte = pmap_pde_to_pte(pde, sva);
-				KASSERT((*pte & PG_V) != 0,
-				    ("pmap_advise: invalid PTE"));
-				pmap_remove_pte(pmap, pte, sva, *pde, NULL,
-				    &lock);
-				anychanged = TRUE;
-			}
+			// if ((oldpde & PG_W) == 0) {
+			// 	pte = pmap_pde_to_pte(pde, sva);
+			// 	KASSERT((*pte & PG_V) != 0,
+			// 	    ("pmap_advise: invalid PTE"));
+			// 	pmap_remove_pte(pmap, pte, sva, *pde, NULL,
+			// 	    &lock);
+			// 	anychanged = TRUE;
+			// }
+			/* patch for MADV_FREE from svn 350191 */	
+ 			if ((oldpde & PG_W) == 0) {
+				va = va_next;
+				if (va > eva)
+					va = eva;
+				va -= PAGE_SIZE;
+				KASSERT(va >= sva, ("pmap_advise: xxx"));
+				pte = pmap_pde_to_pte(pde, va);
+ 				KASSERT((*pte & PG_V) != 0,
+ 				    ("pmap_advise: invalid PTE"));
+				pmap_remove_pte(pmap, pte, va, *pde, NULL,
+ 				    &lock);
+ 				anychanged = TRUE;
+ 			}
 			if (lock != NULL)
 				rw_wunlock(lock);
 		}
diff --git a/sys/amd64/amd64/support.S b/sys/amd64/amd64/support.S
index 6f7dc167a29..158af86237b 100644
--- a/sys/amd64/amd64/support.S
+++ b/sys/amd64/amd64/support.S
@@ -96,6 +96,31 @@ ENTRY(sse2_pagezero)
         ret
 END(sse2_pagezero)
 
+
+/* va: rdi, pagesize: rsi */
+ENTRY(sse2_pagezero_chunk)
+        PUSH_FRAME_POINTER
+        addq    %rsi,%rdi
+        neg		%rsi
+        xorl    %eax,%eax
+        jmp     1f
+        /*
+         * The loop takes 29 bytes.  Ensure that it doesn`t cross a 32-byte
+         * cache line.
+         */
+        .p2align 5,0x90
+1:
+        movnti  %rax,(%rdi,%rsi)
+        movnti  %rax,8(%rdi,%rsi)
+        movnti  %rax,16(%rdi,%rsi)
+        movnti  %rax,24(%rdi,%rsi)
+        addq    $32,%rsi
+        jne     1b
+        sfence
+        POP_FRAME_POINTER
+        ret
+END(sse2_pagezero_chunk)
+
 ENTRY(bcmp)
 	PUSH_FRAME_POINTER
 	movq	%rdx,%rcx
@@ -267,17 +292,22 @@ ENTRY(copyout)
 	ja	copyout_fault
 
 	xchgq	%rdi,%rsi
-	/* bcopy(%rsi, %rdi, %rdx) */
 	movq	%rdx,%rcx
 
-	shrq	$3,%rcx
-	rep
-	movsq
-	movb	%dl,%cl
-	andb	$7,%cl
+	/* erms */
 	rep
 	movsb
 
+	/* bcopy(%rsi, %rdi, %rdx) */
+
+	# shrq	$3,%rcx
+	# rep
+	# movsq
+	# movb	%dl,%cl
+	# andb	$7,%cl
+	# rep
+	# movsb
+
 done_copyout:
 	xorl	%eax,%eax
 	movq	PCPU(CURPCB),%rdx
@@ -317,15 +347,22 @@ ENTRY(copyin)
 
 	xchgq	%rdi,%rsi
 	movq	%rdx,%rcx
-	movb	%cl,%al
-	shrq	$3,%rcx				/* copy longword-wise */
-	rep
-	movsq
-	movb	%al,%cl
-	andb	$7,%cl				/* copy remaining bytes */
+
+	/* erms */
 	rep
 	movsb
 
+	# movb	%cl,%al
+	# shrq	$3,%rcx				/* copy longword-wise */
+	# rep
+	# movsq
+	# movb	%al,%cl
+	# andb	$7,%cl				/* copy remaining bytes */
+	# rep
+	# movsb
+
+
+
 done_copyin:
 	xorl	%eax,%eax
 	movq	PCPU(CURPCB),%rdx
diff --git a/sys/amd64/include/md_var.h b/sys/amd64/include/md_var.h
index 336ebc68fac..1afd9e88ffc 100644
--- a/sys/amd64/include/md_var.h
+++ b/sys/amd64/include/md_var.h
@@ -68,6 +68,8 @@ void	fsbase_load_fault(void) __asm(__STRING(fsbase_load_fault));
 void	gsbase_load_fault(void) __asm(__STRING(gsbase_load_fault));
 void	fpstate_drop(struct thread *td);
 void	pagezero(void *addr);
+void	sse2_pagezero(void *addr);
+void	sse2_pagezero_chunk(void *addr, int pagesize);
 void	setidt(int idx, alias_for_inthand_t *func, int typ, int dpl, int ist);
 struct savefpu *get_pcb_user_save_td(struct thread *td);
 struct savefpu *get_pcb_user_save_pcb(struct pcb *pcb);
diff --git a/sys/amd64/include/pmap.h b/sys/amd64/include/pmap.h
index dcb94f81f32..71609de0138 100644
--- a/sys/amd64/include/pmap.h
+++ b/sys/amd64/include/pmap.h
@@ -117,8 +117,10 @@
  * Promotion to a 2MB (PDE) page mapping requires that the corresponding 4KB
  * (PTE) page mappings have identical settings for the following fields:
  */
+// #define	PG_PTE_PROMOTE	(PG_NX | PG_MANAGED | PG_W | PG_G | PG_PTE_CACHE | \
+// 	    PG_M | PG_A | PG_U | PG_RW | PG_V)
 #define	PG_PTE_PROMOTE	(PG_NX | PG_MANAGED | PG_W | PG_G | PG_PTE_CACHE | \
-	    PG_M | PG_A | PG_U | PG_RW | PG_V)
+	    PG_M | PG_U | PG_RW | PG_V)
 
 /*
  * Page Protection Exception bits
diff --git a/sys/dev/hwpmc/hwpmc_intel.c b/sys/dev/hwpmc/hwpmc_intel.c
index edbf72dff4f..5374af30190 100644
--- a/sys/dev/hwpmc/hwpmc_intel.c
+++ b/sys/dev/hwpmc/hwpmc_intel.c
@@ -179,10 +179,10 @@ pmc_intel_initialize(void)
 			cputype = PMC_CPU_INTEL_IVYBRIDGE_XEON;
 			nclasses = 3;
 			break;
-			/* Intel Skylake */
+			/* Skylake */
 		case 0x4e:
 		case 0x5e:
-			/* Intel Kabylake */
+			/* Kabylake */
 		case 0x8E:
 		case 0x9E:
 			cputype = PMC_CPU_INTEL_SKYLAKE;
diff --git a/sys/vm/pmap.h b/sys/vm/pmap.h
index 6033f37b12e..21b320d1992 100644
--- a/sys/vm/pmap.h
+++ b/sys/vm/pmap.h
@@ -166,6 +166,7 @@ void		 pmap_unwire(pmap_t pmap, vm_offset_t start, vm_offset_t end);
 void		 pmap_zero_page(vm_page_t);
 void		 pmap_zero_page_area(vm_page_t, int off, int size);
 void		 pmap_zero_page_idle(vm_page_t);
+void		 pmap_zero_pages_idle(vm_page_t, int npages);
 
 #define	pmap_resident_count(pm)	((pm)->pm_stats.resident_count)
 #define	pmap_wired_count(pm)	((pm)->pm_stats.wired_count)
diff --git a/sys/vm/vm.h b/sys/vm/vm.h
index e694af9e1d7..a8a77941104 100644
--- a/sys/vm/vm.h
+++ b/sys/vm/vm.h
@@ -154,5 +154,23 @@ void swap_release(vm_ooffset_t decr);
 void swap_release_by_cred(vm_ooffset_t decr, struct ucred *cred);
 void swapper(void);
 
+#ifdef DEBUG_ASYNCPROMO
+extern int last_request;
+/* global function to check if a reservation has pindex collisions */
+// extern boolean_t vm_reserv_is_deadbeef(vm_reserv_t rv);
+
+/* debugging global variable */
+extern vm_reserv_t rv_to_prepopulate; 
+extern int rv_popidx_to_prepopulate;
+
+extern vm_page_t vm_reserv_get_page(vm_reserv_t rv, int i);
+extern int vm_reserv_get_popind(vm_reserv_t rv, int i);
+extern int vm_reserv_get_popcnt(vm_reserv_t rv);
+extern vm_object_t vm_reserv_get_object(vm_reserv_t rv);
+extern vm_pindex_t vm_reserv_get_pindex(vm_reserv_t rv);
+extern void vm_reserv_show_all_pindex_from_page(vm_page_t m);
+extern int vm_reserv_get_popcnt_from_page(vm_page_t m);
+#endif
+
 #endif				/* VM_H */
 
diff --git a/sys/vm/vm_fault.c b/sys/vm/vm_fault.c
index d4c8887bd3a..0274fad2826 100644
--- a/sys/vm/vm_fault.c
+++ b/sys/vm/vm_fault.c
@@ -113,6 +113,10 @@ __FBSDID("$FreeBSD$");
 
 #define	VM_FAULT_DONTNEED_MIN	1048576
 
+/* definitions for reservation popmap bitvector */
+#define	popmap_nbits	(NBBY * sizeof(u_long))
+#define	popmap_nentries		howmany(512, popmap_nbits)
+
 struct faultstate {
 	vm_page_t m;
 	vm_object_t object;
@@ -132,6 +136,38 @@ static void vm_fault_dontneed(const struct faultstate *fs, vm_offset_t vaddr,
 static void vm_fault_prefault(const struct faultstate *fs, vm_offset_t addra,
 	    int backward, int forward, bool obj_locked);
 
+static int enable_adjpromo = 0;
+SYSCTL_INT(_vm, OID_AUTO, enable_adjpromo, CTLFLAG_RWTUN,
+    &enable_adjpromo, 0, "enable adjacent promotion");
+
+static int adjdist = 4;
+SYSCTL_INT(_vm, OID_AUTO, adjdist, CTLFLAG_RWTUN,
+    &adjdist, 0, "adjacent promo distance");
+
+static int enable_syncpromo = 0;
+SYSCTL_INT(_vm, OID_AUTO, enable_syncpromo, CTLFLAG_RWTUN,
+    &enable_syncpromo, 0, "enable sync promotion");
+
+static int sync_succ = 0;
+SYSCTL_INT(_vm, OID_AUTO, sync_succ, CTLFLAG_RWTUN,
+    &sync_succ, 0, "successful sync promotion");
+
+static int sync_fail = 0;
+SYSCTL_INT(_vm, OID_AUTO, sync_fail, CTLFLAG_RWTUN,
+    &sync_fail, 0, "failed sync promotion");
+
+static int sync_prezero = 0;
+SYSCTL_INT(_vm, OID_AUTO, sync_prezero, CTLFLAG_RWTUN,
+    &sync_prezero, 0, "extra zeroed pages");
+
+static int sync_skipzero = 0;
+SYSCTL_INT(_vm, OID_AUTO, sync_skipzero, CTLFLAG_RWTUN,
+    &sync_skipzero, 0, "pages skipping zero");
+
+static int sync_fault = 0;
+SYSCTL_INT(_vm, OID_AUTO, sync_fault, CTLFLAG_RWTUN,
+    &sync_fault, 0, "synchronous superpage faulting");
+
 static inline void
 release_page(struct faultstate *fs)
 {
@@ -310,6 +346,8 @@ vm_fault_soft_fast(struct faultstate *fs, vm_offset_t vaddr, vm_prot_t prot,
 			if ((flags & PS_ALL_DIRTY) != 0)
 				fault_type |= VM_PROT_WRITE;
 		}
+		// if(psind == 0 && m_super->psind == 1)
+		// 	pmap_pde_p_failures_5 ++;
 	}
 #endif
 	rv = pmap_enter(fs->map->pmap, vaddr, m_map, prot, fault_type |
@@ -525,11 +563,15 @@ vm_fault_hold(vm_map_t map, vm_offset_t vaddr, vm_prot_t fault_type,
 	struct faultstate fs;
 	struct vnode *vp;
 	vm_object_t next_object, retry_object;
+	vm_page_t m_ret, m_super; //, m_tmp;
+	vm_paddr_t rv_pa, rv_pa_end;
 	vm_offset_t e_end, e_start;
-	vm_pindex_t retry_pindex;
+	vm_pindex_t retry_pindex, rv_pindex;
 	vm_prot_t prot, retry_prot;
+	u_long popmap[popmap_nentries];
 	int ahead, alloc_req, behind, cluster_offset, error, era, faultcount;
-	int locked, nera, result, rv;
+	int locked, nera, result, rv, i, next_i, psind;
+	int rightdist, leftdist;
 	u_char behavior;
 	boolean_t wired;	/* Passed by reference. */
 	bool dead, hardfault, is_first_object_locked;
@@ -1158,6 +1200,9 @@ RetryFault:;
 			 */
 			vm_object_pip_wakeup(fs.object);
 			VM_OBJECT_WUNLOCK(fs.object);
+			if (faultcount == 0)
+				faultcount = 1;
+			
 			/*
 			 * Only use the new page below...
 			 */
@@ -1248,14 +1293,306 @@ RetryFault:;
 	 */
 	KASSERT(fs.m->valid == VM_PAGE_BITS_ALL,
 	    ("vm_fault: page %p partially invalid", fs.m));
-	VM_OBJECT_WUNLOCK(fs.object);
 
-	/*
-	 * Put this page into the physical map.  We had to do the unlock above
-	 * because pmap_enter() may sleep.  We don't put the page
-	 * back on the active queue until later so that the pageout daemon
-	 * won't find it (yet).
-	 */
+	/* assume mapping a 4KB page */
+	psind = 0;
+
+#if defined(__amd64__) && VM_NRESERVLEVEL > 0
+	/* try adj promo */
+	if(enable_adjpromo && fs.m != NULL &&
+	    !wired && (fault_flags & VM_FAULT_WIRE) == 0 &&
+		fs.object != NULL &&
+		fs.object->type == OBJT_DEFAULT &&
+	    fs.object->backing_object == NULL &&
+	    vm_reserv_satisfy_adj_promotion(fs.m))
+	{
+		rv_pindex = vm_reserv_pindex_from_page(fs.m);
+
+		/* alloc segment by segment */
+		vm_reserv_copy_popmap_from_page(fs.m, popmap);
+		rv_pa = VM_PAGE_TO_PHYS(fs.m) - ((fs.pindex - rv_pindex) << PAGE_SHIFT);
+		rv_pa_end = rv_pa + NBPDR;
+
+		/* utilize 1 cacheline popmap bitvector */
+		i = fs.pindex - rv_pindex;
+		/* 0 -- leftdist -- i -- rightdist -- 512 
+		 */
+		rightdist = i + 1;
+		while(rightdist < 512 
+			&& rightdist - i - 1 <= adjdist 
+			&& (popmap[rightdist / popmap_nbits] 
+			& (1UL << (rightdist % popmap_nbits))) == 0) 
+			++rightdist;
+
+		leftdist = i - 1;
+		while(leftdist > 0 
+			&& i - leftdist - 1 <= adjdist 
+			&& (popmap[leftdist / popmap_nbits] 
+			& (1UL << (leftdist % popmap_nbits))) == 0) 
+			--leftdist;
+
+
+		/* Now let's pre allocate [leftdist+1,i-1], [i+1,rightdist-1] */
+		if(i - 1 - leftdist > 0)
+		{
+
+			/* may try not to busy the page */
+			m_ret = vm_page_alloc_contig(fs.object, rv_pindex + leftdist + 1, 
+				VM_ALLOC_NORMAL | VM_ALLOC_RESERVONLY | VM_ALLOC_ZERO | VM_ALLOC_NOBUSY,
+				i - 1 - leftdist,
+				rv_pa, rv_pa_end,
+				PAGE_SIZE, NBPDR, VM_MEMATTR_DEFAULT);
+
+			if(m_ret != NULL)
+			{
+				/* call sse2_pagezero next_i-i times, no PG_ZERO should be considered */
+				// pmap_zero_pages_idle(m_ret, i - 1 - leftdist);
+				pmap_zero_page_area(m_ret, 0, (i - 1 - leftdist) << 12);
+				sync_prezero += i - 1 - leftdist;
+
+				/* pagers are hot in cache, validate and activate them */
+				vm_page_activate_and_validate_pages(m_ret, i - 1 - leftdist);
+			}
+			else
+				/* abort if allocation failed */
+				goto syncpromo_failed;
+		}
+		if(rightdist - 1 - i > 0)
+		{
+
+			/* may try not to busy the page */
+			m_ret = vm_page_alloc_contig(fs.object, rv_pindex + i + 1, 
+				VM_ALLOC_NORMAL | VM_ALLOC_RESERVONLY | VM_ALLOC_ZERO | VM_ALLOC_NOBUSY,
+				rightdist - 1 - i,
+				rv_pa, rv_pa_end,
+				PAGE_SIZE, NBPDR, VM_MEMATTR_DEFAULT);
+
+			if(m_ret != NULL)
+			{
+				/* call sse2_pagezero next_i-i times, no PG_ZERO should be considered */
+				// pmap_zero_pages_idle(m_ret, rightdist - 1 - i);
+				pmap_zero_page_area(m_ret, 0, (rightdist - 1 - i) << 12);
+				sync_prezero += rightdist - 1 - i;
+
+				/* pages are hot in cache, validate and activate them */
+				vm_page_activate_and_validate_pages(m_ret, rightdist - 1 - i);
+			}
+			else
+				/* abort if allocation failed */
+				goto syncpromo_failed;
+		}
+	}
+
+	/* before releasing vm_object lock , try to do sync promotion */
+	if(enable_syncpromo && fs.m != NULL &&
+	    !wired && (fault_flags & VM_FAULT_WIRE) == 0 &&
+		fs.object != NULL &&
+		fs.object->type == OBJT_DEFAULT &&
+	    fs.object->backing_object == NULL &&
+	    vm_reserv_satisfy_sync_promotion(fs.m))
+	{
+		rv_pindex = vm_reserv_pindex_from_page(fs.m);
+
+		/* alloc page by page */
+		// for(i = 0; i < 512; i ++)
+		// 	if(vm_reserv_popmap_is_clear(fs.m, i))
+		// 	{
+		// 		m_sync = vm_page_alloc(fs.object, rv_pindex + i, 
+		// 			VM_ALLOC_NORMAL | VM_ALLOC_RESERVONLY | VM_ALLOC_ZERO);
+		// 		if(m_sync == NULL)
+		// 		{
+		// 			/* stop doing sync populating */
+		// 			goto syncpromo_failed;
+		// 		}
+		// 		else
+		// 		{
+		// 			if((m_sync->flags & PG_ZERO) == 0)
+		// 			{
+		// 				sync_prezero ++;
+		// 				pmap_zero_page_idle(m_sync);
+		// 				m_sync->flags &= PG_ZERO;
+		// 			}
+		// 			else
+		// 				sync_skipzero ++;
+		// 			m_sync->valid = VM_PAGE_BITS_ALL;
+		// 			vm_page_xunbusy(m_sync);
+		// 		}
+		// 	}
+
+		/* alloc segment by segment */
+		vm_reserv_copy_popmap_from_page(fs.m, popmap);
+		rv_pa = VM_PAGE_TO_PHYS(fs.m) - ((fs.pindex - rv_pindex) << PAGE_SHIFT);
+		rv_pa_end = rv_pa + NBPDR;
+
+		/* utilize 1 cacheline popmap bitvector */
+		i = 0;
+		while(i < 512)
+		{
+			/* find next i with popmap[i] cleared */
+			while(i < 512 && (popmap[i / popmap_nbits] & 
+				(1UL << (i % popmap_nbits))) != 0) ++i;
+			next_i = i;
+			/* find next next_i with popmap[next_i] set */
+			while(next_i < 512 && (popmap[next_i / popmap_nbits] & 
+				(1UL << (next_i % popmap_nbits))) == 0) ++next_i;
+
+			if(i < next_i)
+			{
+				/* may try not to busy the page */
+				m_ret = vm_page_alloc_contig(fs.object, rv_pindex + i, 
+					VM_ALLOC_NORMAL | VM_ALLOC_RESERVONLY | VM_ALLOC_ZERO | VM_ALLOC_NOBUSY,
+					next_i - i,
+					rv_pa, rv_pa_end,
+					PAGE_SIZE, NBPDR, VM_MEMATTR_DEFAULT);
+
+				if(m_ret != NULL)
+				{
+					/* call sse2_pagezero next_i-i times, no PG_ZERO should be considered */
+					pmap_zero_pages_idle(m_ret, next_i - i);
+					sync_prezero += next_i - i;
+
+					/* pages are hot in cache, validate and activate them */
+					vm_page_activate_and_validate_pages(m_ret, next_i - i);
+					// for(m_tmp = m_ret; m_tmp < &m_ret[next_i - i]; m_tmp ++)
+					// 	m_tmp->valid = VM_PAGE_BITS_ALL;
+				}
+				else
+					/* abort if allocation failed */
+					goto syncpromo_failed;
+			}
+			i = next_i;
+		}
+		// uprintf("finished allocation\n");
+
+		// i = vm_reserv_get_next_clear_index(fs.m, 0); 
+		// rv_pa = VM_PAGE_TO_PHYS(fs.m) - ((fs.pindex - rv_pindex) << PAGE_SHIFT);
+		// rv_pa_end = rv_pa + NBPDR;
+		
+		// while(i < 512)
+		// {
+		// 	next_i = vm_reserv_get_next_set_index(fs.m, i);
+		// 	m_ret = vm_page_alloc_contig(fs.object, rv_pindex + i, 
+		// 		VM_ALLOC_NORMAL | VM_ALLOC_RESERVONLY | VM_ALLOC_ZERO,
+		// 		next_i - i,
+		// 		rv_pa, rv_pa_end,
+		// 		PAGE_SIZE, NBPDR, VM_MEMATTR_DEFAULT);
+
+		// 	if(m_ret != NULL)
+		// 	{
+		// 		/* call sse2_pagezero next_i-i times, no PG_ZERO should be considered */
+		// 		pmap_zero_pages_idle(m_ret, next_i - i);
+		// 		sync_prezero += next_i - i;
+
+		// 		/* scan pages in order to skip */
+		// 		for(m_sync = m_ret; m_sync < &m_ret[next_i - i]; m_sync ++)
+		// 		{
+		// 			 try not to affect the cache because this is not the
+		// 			 * faulted page
+					 
+		// 			// if((m_sync->flags & PG_ZERO) == 0)
+		// 			// {
+		// 			// 	sync_prezero ++;
+		// 			// 	pmap_zero_page_idle(m_sync);
+		// 			// 	// m_sync->flags &= PG_ZERO;
+		// 			// }
+		// 			// else
+		// 			// 	sync_skipzero ++;
+
+		// 			m_sync->valid = VM_PAGE_BITS_ALL;
+		// 			/* put it in the active queue */
+		// 			vm_page_lock(m_sync);
+		// 			vm_page_activate(m_sync);
+		// 			vm_page_unlock(m_sync);
+		// 			vm_page_xunbusy(m_sync);
+		// 		}
+		// 		i = vm_reserv_get_next_clear_index(fs.m, next_i);
+		// 	}
+		// 	else
+		// 		/* abort if allocation failed */
+		// 		goto syncpromo_failed;
+		// }
+
+		if(i == 512)
+		{
+
+			CTR2(KTR_VM, 
+				"sync superpage promotion succeeded, pid %d (%s)\n",
+				    curproc->p_pid, curproc->p_comm);
+			sync_succ ++;
+
+			/*
+			 * Put this page into the physical map.  We had to do the unlock above
+			 * because pmap_enter() may sleep.  We don't put the page
+			 * back on the active queue until later so that the pageout daemon
+			 * won't find it (yet).
+			 */
+
+			/* check anonymous superpage mapping legitmacy if fully-populated */
+			psind = 0;
+			if ((fs.m->flags & PG_FICTITIOUS) == 0 &&
+			    (m_super = vm_reserv_to_superpage(fs.m)) != NULL &&
+			    rounddown2(vaddr, pagesizes[m_super->psind]) >= fs.entry->start &&
+			    roundup2(vaddr + 1, pagesizes[m_super->psind]) <= fs.entry->end &&
+			    (vaddr & (pagesizes[m_super->psind] - 1)) == (VM_PAGE_TO_PHYS(fs.m) &
+			    (pagesizes[m_super->psind] - 1)) &&
+			    pmap_ps_enabled(fs.map->pmap)) 
+			{
+				psind = m_super->psind;
+				vaddr = rounddown2(vaddr, pagesizes[psind]);
+			}
+			if(psind == 1)
+			{
+				/* You are not wired here */
+				rv = pmap_enter(fs.map->pmap, vaddr, m_super, prot,
+				    fault_type | PMAP_ENTER_NOSLEEP | (wired ? PMAP_ENTER_WIRED : 0), 1);
+				if(rv == KERN_SUCCESS)
+				{
+					/* Succeeded to map a superpage */
+					sync_fault ++;
+					goto skip_pmap;
+				}
+			}
+		}
+		else
+		{
+syncpromo_failed:
+			vm_reserv_mark_bad(fs.m);
+			sync_fail ++;
+			// uprintf("fail allocation %d - %d\n", i, next_i);
+		}
+
+		/* activate all allocated pages: 
+		 * m_super[0] -> m_super[next_i] 
+		 * m_super might be NULL because the allocation could fail
+		 */
+		// next_i = i;
+
+		// m_super = fs.m - (fs.pindex - rv_pindex);
+		// if(m_super != NULL)
+		// {
+			/* pages m_super -- m_super + 512 are all locked */
+			// vm_page_lock(m_super);
+			// uprintf("going to activate\n");
+			// for(i = 0; i < next_i; i ++)
+			// 	if((popmap[i / popmap_nbits] & 
+			// 		(1UL << (i % popmap_nbits))) == 0)
+			// 	{
+					/* port code from vm_page_xunbusy_locked */
+					// atomic_store_rel_int(&m_super[i].busy_lock, VPB_UNBUSIED);
+					// wakeup(&m_super[i]);
+				// 	m_super[i].valid = VM_PAGE_BITS_ALL;
+				// }
+			// vm_page_activate_super(m_super, fs.m);
+			// uprintf("finished activation\n");
+			/* validate all pre-allocated pages, fs.m will be skipped */
+			// uprintf("finished xunbusy\n");
+			// vm_page_unlock(m_super);
+		// }
+		// uprintf("Done with all allocation stuff\n");
+	}
+#endif
+
+	VM_OBJECT_WUNLOCK(fs.object);
 	pmap_enter(fs.map->pmap, vaddr, fs.m, prot,
 	    fault_type | (wired ? PMAP_ENTER_WIRED : 0), 0);
 	if (faultcount != 1 && (fault_flags & VM_FAULT_WIRE) == 0 &&
@@ -1264,6 +1601,8 @@ RetryFault:;
 		    faultcount > 0 ? behind : PFBAK,
 		    faultcount > 0 ? ahead : PFFOR, false);
 	VM_OBJECT_WLOCK(fs.object);
+
+skip_pmap:
 	vm_page_lock(fs.m);
 
 	/*
diff --git a/sys/vm/vm_page.c b/sys/vm/vm_page.c
index 5695fa7a5c9..3728b227a7b 100644
--- a/sys/vm/vm_page.c
+++ b/sys/vm/vm_page.c
@@ -126,6 +126,9 @@ __FBSDID("$FreeBSD$");
  *	page structure.
  */
 
+#ifdef DEBUG_ASYNCPROMO
+int last_request;
+#endif
 struct vm_domain vm_dom[MAXMEMDOM];
 struct mtx_padalign __exclusive_cache_line vm_page_queue_free_mtx;
 
@@ -145,6 +148,10 @@ static int pa_tryrelock_restart;
 SYSCTL_INT(_vm, OID_AUTO, tryrelock_restart, CTLFLAG_RD,
     &pa_tryrelock_restart, 0, "Number of tryrelock restarts");
 
+static int relax = 0;
+SYSCTL_INT(_vm, OID_AUTO, relax, CTLFLAG_RWTUN,
+    &relax, 0, "relax vm_page_ps_test");
+
 static TAILQ_HEAD(, vm_page) blacklist_head;
 static int sysctl_vm_page_blacklist(SYSCTL_HANDLER_ARGS);
 SYSCTL_PROC(_vm, OID_AUTO, page_blacklist, CTLTYPE_STRING | CTLFLAG_RD |
@@ -165,8 +172,8 @@ static int vm_page_insert_after(vm_page_t m, vm_object_t object,
     vm_pindex_t pindex, vm_page_t mpred);
 static void vm_page_insert_radixdone(vm_page_t m, vm_object_t object,
     vm_page_t mpred);
-static int vm_page_reclaim_run(int req_class, u_long npages, vm_page_t m_run,
-    vm_paddr_t high);
+// static int vm_page_reclaim_run(int req_class, u_long npages, vm_page_t m_run,
+//     vm_paddr_t high);
 static int vm_page_alloc_fail(vm_object_t object, int req);
 
 SYSINIT(vm_page, SI_SUB_VM, SI_ORDER_SECOND, vm_page_init_fakepg, NULL);
@@ -1421,6 +1428,25 @@ vm_page_find_least(vm_object_t object, vm_pindex_t pindex)
 	return (m);
 }
 
+/*
+ *	vm_page_find_most:
+ *
+ *	Returns the page associated with the object with biggest pindex
+ *	less than or equal to the parameter pindex, or NULL.
+ *
+ *	The object must be locked.
+ */
+vm_page_t
+vm_page_find_most(vm_object_t object, vm_pindex_t pindex)
+{
+	vm_page_t m;
+
+	VM_OBJECT_ASSERT_LOCKED(object);
+	if ((m = TAILQ_LAST(&object->memq, pglist)) != NULL && m->pindex > pindex)
+		m = vm_radix_lookup_le(&object->rtree, pindex);
+	return (m);
+}
+
 /*
  * Returns the given page's successor (by pindex) within the object if it is
  * resident; if none is found, NULL is returned.
@@ -1651,25 +1677,55 @@ vm_page_alloc_after(vm_object_t object, vm_pindex_t pindex, int req,
 		/*
 		 * Can we allocate the page from a reservation?
 		 */
+		
 #if VM_NRESERVLEVEL > 0
 		if (object == NULL || (object->flags & (OBJ_COLORED |
 		    OBJ_FICTITIOUS)) != OBJ_COLORED || (m =
-		    vm_reserv_alloc_page(object, pindex, mpred)) == NULL)
-#endif
-		{
-			/*
-			 * If not, allocate it from the free page queues.
+		    vm_reserv_alloc_page(object, pindex, mpred)) == NULL
+		    )
+
+			/* 
+			 * [asyncpromo] 
+			 * Must we alloc the page from a reservation?
+			 * check if req & VM_ALLOC_RESERVONLY == 0
+			 * define a else which unlocks the freequeue lock and return
 			 */
-			m = vm_phys_alloc_pages(object != NULL ?
-			    VM_FREEPOOL_DEFAULT : VM_FREEPOOL_DIRECT, 0);
-#if VM_NRESERVLEVEL > 0
-			if (m == NULL && vm_reserv_reclaim_inactive()) {
+		{
+		    if((req & VM_ALLOC_RESERVONLY) == 0)
+#endif
+			{
+				/*
+				 * If not, allocate it from the free page queues.
+				 */
 				m = vm_phys_alloc_pages(object != NULL ?
-				    VM_FREEPOOL_DEFAULT : VM_FREEPOOL_DIRECT,
-				    0);
-			}
+				    VM_FREEPOOL_DEFAULT : VM_FREEPOOL_DIRECT, 0);
+#if VM_NRESERVLEVEL > 0
+				if (m == NULL && vm_reserv_reclaim_inactive()) {
+					m = vm_phys_alloc_pages(object != NULL ?
+					    VM_FREEPOOL_DEFAULT : VM_FREEPOOL_DIRECT,
+					    0);
+				}
 #endif
+			}
+
+#if VM_NRESERVLEVEL > 0
+			/* 
+			 * [asyncpromo]
+			 * Seems that asyncpromo is trying to alloc a page
+			 * from a broken reservation
+			 * 
+			 * unlock the free page queue lock 
+			 * (like vm_page alloc_fail) and return NULL
+			 * Tell asyncpromo to avoid unnecessary forward effort 
+			 * to zero the remaining pages
+			 */
+			else {
+				mtx_unlock(&vm_page_queue_free_mtx);
+				return (NULL);
+			}
 		}
+#endif
+
 	} else {
 		/*
 		 * Not allocatable, give up.
@@ -1718,6 +1774,9 @@ vm_page_alloc_after(vm_object_t object, vm_pindex_t pindex, int req,
 	m->act_count = 0;
 
 	if (object != NULL) {
+#ifdef DEBUG_ASYNCPROMO
+		last_request = req;
+#endif
 		if (vm_page_insert_after(m, object, pindex, mpred)) {
 			pagedaemon_wakeup();
 			if (req & VM_ALLOC_WIRED) {
@@ -1854,11 +1913,43 @@ vm_page_alloc_contig(vm_object_t object, vm_pindex_t pindex, int req,
 		    (m_ret = vm_reserv_alloc_contig(object, pindex, npages,
 		    low, high, alignment, boundary, mpred)) == NULL)
 #endif
-			/*
-			 * If not, allocate them from the free page queues.
+			/* 
+			 * [syncpromo] 
+			 * Must we alloc the pages from a reservation?
+			 * check if req & VM_ALLOC_RESERVONLY == 0
+			 * abort if we must allocate from a reservation
 			 */
-			m_ret = vm_phys_alloc_contig(npages, low, high,
-			    alignment, boundary);
+		{
+
+#if VM_NRESERVLEVEL > 0
+		    if((req & VM_ALLOC_RESERVONLY) == 0)
+#endif
+				/*
+				 * If not, allocate them from the free page queues.
+				 */
+				m_ret = vm_phys_alloc_contig(npages, low, high,
+				    alignment, boundary);
+
+#if VM_NRESERVLEVEL > 0
+			else 
+			{
+				/* 
+				 * [syncpromo]
+				 * Seems that asyncpromo is trying to alloc a page
+				 * from a broken reservation
+				 * 
+				 * unlock the free page queue lock 
+				 * (like vm_page alloc_fail) and return NULL
+				 * Tell asyncpromo to avoid unnecessary forward effort 
+				 * to zero the remaining pages
+				 *
+				 * Do not squeeze the vm_phys_alloc_contig but fail
+				 */
+				mtx_unlock(&vm_page_queue_free_mtx);
+				return (NULL);
+			}
+		}
+#endif
 	} else {
 		if (vm_page_alloc_fail(object, req))
 			goto again;
@@ -2270,7 +2361,7 @@ vm_page_scan_contig(u_long npages, vm_page_t m_start, vm_page_t m_end,
  *
  *	"req_class" must be an allocation class.
  */
-static int
+int
 vm_page_reclaim_run(int req_class, u_long npages, vm_page_t m_run,
     vm_paddr_t high)
 {
@@ -2809,6 +2900,71 @@ vm_page_activate(vm_page_t m)
 	}
 }
 
+/* 
+ * activate 512 pages for a superpage
+ * the m_super page lock is held (a hack for current page lock implementation)
+ * skip m_skip which will be activated later by vm_fault_hold
+ * prefaulted pages are not wired.
+ */
+void
+vm_page_activate_super(vm_page_t m_super, vm_page_t m_skip)
+{
+	struct vm_pagequeue *pq;
+	vm_page_t m_tmp;
+
+	vm_page_lock_assert(m_super, MA_OWNED);
+
+	/* all superpage belongs to the same vm_domain */
+	pq = &vm_phys_domain(m_super)->vmd_pagequeues[PQ_ACTIVE];
+	vm_pagequeue_lock(pq);
+	for(m_tmp = m_super; m_tmp < &m_super[512]; m_tmp++)
+	{
+		/* most pages are not faulted so simply scan over all pages */
+		if(m_tmp->queue == PQ_NONE && m_tmp->valid == VM_PAGE_BITS_ALL && m_tmp != m_skip)
+		{
+			if (m_tmp->act_count < ACT_INIT)
+				m_tmp->act_count = ACT_INIT;
+			// vm_page_enqueue(PQ_ACTIVE, m_tmp);
+			m_tmp->queue = PQ_ACTIVE;
+			TAILQ_INSERT_TAIL(&pq->pq_pl, m_tmp, plinks.q);
+			vm_pagequeue_cnt_inc(pq);
+		}
+	}
+	vm_pagequeue_unlock(pq);
+}
+
+
+/* 
+ * activate pages in a range
+ */
+void
+vm_page_activate_and_validate_pages(vm_page_t m_left, int npages)
+{
+	struct vm_pagequeue *pq;
+	vm_page_t m_tmp;
+
+	vm_page_lock_assert(m_left, MA_OWNED);
+
+	/* all superpage belongs to the same vm_domain */
+	pq = &vm_phys_domain(m_left)->vmd_pagequeues[PQ_ACTIVE];
+	vm_pagequeue_lock(pq);
+	for(m_tmp = m_left; m_tmp < &m_left[npages]; m_tmp++)
+	{
+		/* validate this page */
+		m_tmp->valid = VM_PAGE_BITS_ALL;
+		/* most pages are not faulted so simply scan over all pages */
+		if (m_tmp->act_count < ACT_INIT)
+			m_tmp->act_count = ACT_INIT;
+		if (m_tmp->queue != PQ_NONE)
+			vm_page_dequeue(m_tmp);
+		// vm_page_enqueue(PQ_ACTIVE, m_tmp);
+		m_tmp->queue = PQ_ACTIVE;
+		TAILQ_INSERT_TAIL(&pq->pq_pl, m_tmp, plinks.q);
+		vm_pagequeue_cnt_inc(pq);
+	}
+	vm_pagequeue_unlock(pq);
+}
+
 /*
  *	vm_page_free_wakeup:
  *
@@ -3722,6 +3878,7 @@ vm_page_is_valid(vm_page_t m, int base, int size)
 	return (m->valid != 0 && (m->valid & bits) == bits);
 }
 
+// int ps_fail[4] = {0};
 /*
  * Returns true if all of the specified predicates are true for the entire
  * (super)page and false otherwise.
@@ -3731,13 +3888,14 @@ vm_page_ps_test(vm_page_t m, int flags, vm_page_t skip_m)
 {
 	vm_object_t object;
 	int i, npages;
+	boolean_t skip_dirty;
 
 	object = m->object;
 	if (skip_m != NULL && skip_m->object != object)
 		return (false);
 	VM_OBJECT_ASSERT_LOCKED(object);
 	npages = atop(pagesizes[m->psind]);
-
+	skip_dirty = relax && (object->type == OBJT_DEFAULT && object->backing_object == NULL);
 	/*
 	 * The physically contiguous pages that make up a superpage, i.e., a
 	 * page with a page size index ("psind") greater than zero, will
@@ -3746,12 +3904,20 @@ vm_page_ps_test(vm_page_t m, int flags, vm_page_t skip_m)
 	for (i = 0; i < npages; i++) {
 		/* Always test object consistency, including "skip_m". */
 		if (m[i].object != object)
+		{
+			// ps_fail[0] ++;
+			// printf("ps_test: [%d %d %d %d]\n", ps_fail[0], ps_fail[1], ps_fail[2], ps_fail[3]);
 			return (false);
+		}
 		if (&m[i] == skip_m)
 			continue;
 		if ((flags & PS_NONE_BUSY) != 0 && vm_page_busied(&m[i]))
+		{
+			// ps_fail[1] ++;
+			// printf("ps_test: [%d %d %d %d]\n", ps_fail[0], ps_fail[1], ps_fail[2], ps_fail[3]);
 			return (false);
-		if ((flags & PS_ALL_DIRTY) != 0) {
+		}
+		if (!skip_dirty && (flags & PS_ALL_DIRTY) != 0) {
 			/*
 			 * Calling vm_page_test_dirty() or pmap_is_modified()
 			 * might stop this case from spuriously returning
@@ -3759,11 +3925,20 @@ vm_page_ps_test(vm_page_t m, int flags, vm_page_t skip_m)
 			 * on the object containing "m[i]".
 			 */
 			if (m[i].dirty != VM_PAGE_BITS_ALL)
+			{
+				// ps_fail[2] ++;
+				// printf("ps_test: [%d %d %d %d]\n", ps_fail[0], ps_fail[1], ps_fail[2], ps_fail[3]);
 				return (false);
+			}
 		}
 		if ((flags & PS_ALL_VALID) != 0 &&
 		    m[i].valid != VM_PAGE_BITS_ALL)
-			return (false);
+			{
+
+				// ps_fail[3] ++;
+				// printf("ps_test: [%d %d %d %d]\n", ps_fail[0], ps_fail[1], ps_fail[2], ps_fail[3]);
+				return (false);
+			}
 	}
 	return (true);
 }
diff --git a/sys/vm/vm_page.h b/sys/vm/vm_page.h
index 9b31ad8f3f8..c2fe680d4f4 100644
--- a/sys/vm/vm_page.h
+++ b/sys/vm/vm_page.h
@@ -422,6 +422,11 @@ vm_page_t PHYS_TO_VM_PAGE(vm_paddr_t pa);
 #define	VM_ALLOC_NOWAIT		0x8000	/* (acfgp) Do not sleep */
 #define	VM_ALLOC_COUNT_SHIFT	16
 #define	VM_ALLOC_COUNT(count)	((count) << VM_ALLOC_COUNT_SHIFT)
+/*
+ * flag dedicated to asyncpromo, the vm_page_alloc must install a free
+ * page from a reservation, otherwise fails.
+ */
+#define VM_ALLOC_RESERVONLY 0x0004
 
 #ifdef M_NOWAIT
 static inline int
@@ -467,6 +472,8 @@ void vm_page_free(vm_page_t m);
 void vm_page_free_zero(vm_page_t m);
 
 void vm_page_activate (vm_page_t);
+void vm_page_activate_super(vm_page_t m_super, vm_page_t m_skip);
+void vm_page_activate_and_validate_pages(vm_page_t m_left, int npages);
 void vm_page_advise(vm_page_t m, int advice);
 vm_page_t vm_page_alloc(vm_object_t, vm_pindex_t, int);
 vm_page_t vm_page_alloc_after(vm_object_t, vm_pindex_t, int, vm_page_t);
@@ -484,6 +491,7 @@ void vm_page_deactivate_noreuse(vm_page_t);
 void vm_page_dequeue(vm_page_t m);
 void vm_page_dequeue_locked(vm_page_t m);
 vm_page_t vm_page_find_least(vm_object_t, vm_pindex_t);
+vm_page_t vm_page_find_most(vm_object_t, vm_pindex_t);
 void vm_page_free_phys_pglist(struct pglist *tq);
 bool vm_page_free_prep(vm_page_t m, bool pagequeue_locked);
 vm_page_t vm_page_getfake(vm_paddr_t paddr, vm_memattr_t memattr);
@@ -500,6 +508,8 @@ void vm_page_putfake(vm_page_t m);
 void vm_page_readahead_finish(vm_page_t m);
 bool vm_page_reclaim_contig(int req, u_long npages, vm_paddr_t low,
     vm_paddr_t high, u_long alignment, vm_paddr_t boundary);
+int vm_page_reclaim_run(int req_class, u_long npages, vm_page_t m_run,
+    vm_paddr_t high);
 void vm_page_reference(vm_page_t m);
 void vm_page_remove (vm_page_t);
 int vm_page_rename (vm_page_t, vm_object_t, vm_pindex_t);
diff --git a/sys/vm/vm_phys.c b/sys/vm/vm_phys.c
index faaf58fb13e..f02785df583 100644
--- a/sys/vm/vm_phys.c
+++ b/sys/vm/vm_phys.c
@@ -396,6 +396,29 @@ sysctl_vm_phys_free(SYSCTL_HANDLER_ARGS)
 	return (error);
 }
 
+
+
+/*
+ * Return the number of available reservation allocations
+ */
+int
+vm_phys_count_order_9(void)
+{
+	struct vm_freelist *fl;
+	int dom, flind, oind, pind;
+	int count = 0;
+
+	for (dom = 0; dom < vm_ndomains; dom++) 
+		for (flind = 0; flind < vm_nfreelists; flind++) 
+			for (oind = VM_NFREEORDER - 1; oind >= 9; oind--) 
+				for (pind = 0; pind < VM_NFREEPOOL; pind++) 
+				{
+					fl = vm_phys_free_queues[dom][flind][pind];
+					count += fl[oind].lcnt << (oind - 9);
+				}
+	return count;
+}
+
 /*
  * Outputs the set of physical memory segments.
  */
@@ -1483,3 +1506,85 @@ DB_SHOW_COMMAND(freepages, db_show_freepages)
 	}
 }
 #endif
+
+
+static void
+vm_phys_prezero_all(void)
+{
+	vm_page_t m, m_tmp, m_first;
+	int order, zeroed;
+
+	mtx_lock(&vm_page_queue_free_mtx);
+
+	for(order = VM_NFREEORDER - 1; order >= 9; order --)
+	{
+		zeroed = 0;
+		m_first = NULL;
+		for(;;)
+		{
+			m = vm_phys_alloc_pages(VM_FREEPOOL_DEFAULT, order);
+
+			/* no resource or finishes scanning */
+			if(m == NULL || m == m_first)
+			{
+				if(m != NULL)
+				 	vm_phys_free_pages(m, order);
+				break;
+			}
+
+			if(m_first == NULL)
+				m_first = m;
+
+			/* the freelist queue of order has been fully scanned */
+			// if((m->flags & PG_ZERO) != 0)
+			// {
+			// 	vm_phys_free_pages(m, order);
+			// 	break;
+			// }
+
+
+			for (m_tmp = m; m_tmp < &m[1 << order]; m_tmp++) 
+			{
+				if ((m_tmp->flags & PG_ZERO) == 0) 
+				{
+					pmap_zero_page_idle(m_tmp);
+					m_tmp->flags |= PG_ZERO;
+					vm_page_zero_count++;
+					cnt_prezero++;
+				}
+				else
+				{
+					/* It is fine, skip and do nothing */
+					// panic("zero pages in contiguous pre-zero");
+				}
+			}
+
+			zeroed ++;
+			vm_phys_free_pages(m, order);
+		}
+
+		printf("Zeroed %d order-%d pages\n", zeroed, order);
+	}
+
+	mtx_unlock(&vm_page_queue_free_mtx);
+}
+
+/*
+ * Allow userspace to directly trigger prezero
+ */
+static int
+debug_vm_prezero_all(SYSCTL_HANDLER_ARGS)
+{
+	int error, i;
+
+	i = 0;
+	error = sysctl_handle_int(oidp, &i, 0, req);
+	if (error)
+		return (error);
+	if (i != 0)
+		vm_phys_prezero_all();
+	return (0);
+}
+
+SYSCTL_PROC(_vm, OID_AUTO, prezero_all, CTLTYPE_INT | CTLFLAG_RW, 0, 0,
+    debug_vm_prezero_all, "I", "set to prezero all chunks le 2MB");
\ No newline at end of file
diff --git a/sys/vm/vm_phys.h b/sys/vm/vm_phys.h
index 8d9fe5758b8..8768922b816 100644
--- a/sys/vm/vm_phys.h
+++ b/sys/vm/vm_phys.h
@@ -74,6 +74,7 @@ vm_page_t vm_phys_alloc_contig(u_long npages, vm_paddr_t low, vm_paddr_t high,
     u_long alignment, vm_paddr_t boundary);
 vm_page_t vm_phys_alloc_freelist_pages(int freelist, int pool, int order);
 vm_page_t vm_phys_alloc_pages(int pool, int order);
+int vm_phys_count_order_9(void);
 boolean_t vm_phys_domain_intersects(long mask, vm_paddr_t low, vm_paddr_t high);
 int vm_phys_fictitious_reg_range(vm_paddr_t start, vm_paddr_t end,
     vm_memattr_t memattr);
diff --git a/sys/vm/vm_radix.c b/sys/vm/vm_radix.c
index 546d316211d..15c34aafe21 100644
--- a/sys/vm/vm_radix.c
+++ b/sys/vm/vm_radix.c
@@ -352,9 +352,43 @@ vm_radix_insert(struct vm_radix *rtree, vm_page_t page)
 	for (;;) {
 		if (vm_radix_isleaf(rnode)) {
 			m = vm_radix_topage(rnode);
-			if (m->pindex == index)
+			if (m->pindex == index) 
+			{
+#ifdef DEBUG_ASYNCPROMO
+				printf("last request is :%x\n", last_request);
+				printf("popcnt of m: %d\n", vm_reserv_get_popcnt_from_page(m));
+				printf("[panic] same page:%d <rtree,insert>: valid:<%d,%d> busy:<%d,%d> object:<%p,%p>\n", 
+					m == page, 
+					m->valid, page->valid,
+					vm_page_xbusied(m), vm_page_xbusied(page),
+					m->object, page->object
+					);
+				// printf("[panic] vm_page_t <%p:%p:%p>, pindex:<%lu,%lu>, popcnt:%d, popmap:%d\n",
+				// 	m, page, vm_reserv_get_page(rv_to_prepopulate, rv_popidx_to_prepopulate),
+				// 	m->pindex, page->pindex,
+				// 	vm_reserv_get_popcnt(rv_to_prepopulate), 
+				// 	vm_reserv_get_popind(rv_to_prepopulate, rv_popidx_to_prepopulate)
+				// 	);
+				// printf("[panic] paddr <%lx:%lx:%lx>\n",
+				// 	m->phys_addr,
+				// 	page->phys_addr,
+				// 	vm_reserv_get_page(rv_to_prepopulate, rv_popidx_to_prepopulate)->phys_addr
+				// 	);
+				// printf("[panic] rv: object[%p] rv[%p] rv->pindex[%lu]\n", 
+				// 	vm_reserv_get_object(rv_to_prepopulate),
+				// 	rv_to_prepopulate,
+				// 	vm_reserv_get_pindex(rv_to_prepopulate));
+
+				// printf("[panic] pindex from m\n");
+				// vm_reserv_show_all_pindex_from_page(m);
+				// printf("[panic] pindex from rv_to_prepopulate\n");
+				// vm_reserv_show_all_pindex_from_page(vm_reserv_get_page(rv_to_prepopulate, rv_popidx_to_prepopulate));
+				// printf("[panic] rv is deadbeef? %d\n", vm_reserv_is_deadbeef(rv_to_prepopulate));
+				// KASSERT(m->pindex != index, "pindex already exists in radix tree\n");
+#endif
 				panic("%s: key %jx is already present",
 				    __func__, (uintmax_t)index);
+			}
 			clev = vm_radix_keydiff(m->pindex, index);
 			tmp = vm_radix_node_get(vm_radix_trimkey(index,
 			    clev + 1), 2, clev);
diff --git a/sys/vm/vm_reserv.c b/sys/vm/vm_reserv.c
index d707aca7fd7..10744fe2206 100644
--- a/sys/vm/vm_reserv.c
+++ b/sys/vm/vm_reserv.c
@@ -61,6 +61,15 @@ __FBSDID("$FreeBSD$");
 #include <vm/vm_radix.h>
 #include <vm/vm_reserv.h>
 
+/* reserv idle daemon which zeros pages inside reservations */
+#include <opt_sched.h>
+#include <sys/proc.h>
+#include <sys/sched.h>
+#include <sys/kthread.h>
+#include <sys/unistd.h>
+
+
+
 /*
  * The reservation system supports the speculative allocation of large physical
  * pages ("superpages").  Speculative allocation enables the fully automatic
@@ -148,6 +157,29 @@ popmap_is_set(popmap_t popmap[], int i)
 	return ((popmap[i / NBPOPMAP] & (1UL << (i % NBPOPMAP))) != 0);
 }
 
+/*
+ * encode inpartpopq, need-to-migrate and marker status in inpartpopq
+ * TODO: 
+ *	1. need an extra thread to migrate detected reservations and use
+ *  clear_migrate(x)
+ */
+#define RV_INPARTPOPQ		0x01
+#define RV_TRANSFERRED		0x02
+#define RV_NEEDMIGRATE 		0x04
+#define RV_MARKER			0x08
+#define RV_BADBOY			0x10
+// #define RV_SKIP				0x20
+#define clear_all(x)		(x = 0)
+#define is_inpartpopq(x)	(x & RV_INPARTPOPQ)
+#define clear_inpartpopq(x)	(x &= ~RV_INPARTPOPQ)
+#define set_inpartpopq(x)   (x |= RV_INPARTPOPQ)
+// #define is_skip(x)			(x & RV_SKIP)
+// #define set_skip(x)			(x |= RV_SKIP)
+// #define clear_skip(x)		(x &= ~RV_SKIP)
+#define settime(rv)			(rv->timestamp = ticks)
+#define need_migrate(x)		(x & RV_NEEDMIGRATE)
+#define clear_migrate(x)	(x &= ~RV_NEEDMIGRATE)
+#define set_migrate(x)		{x |= RV_NEEDMIGRATE;numofdeadbeef ++;}
 /*
  * The reservation structure
  *
@@ -169,10 +201,29 @@ struct vm_reserv {
 	vm_pindex_t	pindex;			/* offset within object */
 	vm_page_t	pages;			/* first page of a superpage */
 	int		popcnt;			/* # of pages in use */
+	int 	timestamp; 		/* timestamp last populated/depopulated */
 	char		inpartpopq;
 	popmap_t	popmap[NPOPMAP];	/* bit vector of used pages */
 };
 
+/* initialize the marker to help asyncpromo scan partpopq */
+struct vm_reserv async_marker, evict_marker, compact_marker;
+
+/* The marker initializer does not assign the memory */
+static void vm_reserv_init_marker(vm_reserv_t rv)
+{
+	bzero(rv, sizeof(*rv));
+	rv->object = NULL;
+	rv->inpartpopq = RV_INPARTPOPQ | RV_MARKER;
+	for(int i = 0; i < VM_LEVEL_0_NPAGES; i ++)
+		popmap_clear(rv->popmap, i);
+}
+/* 
+ * [asyncpromo] 
+ * TAILQ headname for struct vm_reserv 
+ */
+TAILQ_HEAD(rvlist, vm_reserv);
+
 /*
  * The reservation array
  *
@@ -210,6 +261,120 @@ static TAILQ_HEAD(, vm_reserv) vm_rvq_partpop =
 
 static SYSCTL_NODE(_vm, OID_AUTO, reserv, CTLFLAG_RD, 0, "Reservation Info");
 
+/* add statistics which counts how many pages are zeroed in reservation for
+ * early promotion at idle time
+ */
+static int async_prezero = 0, numofdeadbeef = 0, async_skipzero = 0;
+SYSCTL_INT(_vm_reserv, OID_AUTO, async_prezero, CTLFLAG_RD,
+    &async_prezero, 0, "Pages prezeroed for early promotion at idle time");
+SYSCTL_INT(_vm_reserv, OID_AUTO, async_skipzero, CTLFLAG_RD,
+    &async_skipzero, 0, "Pages skipped for zero in async promotion");
+// SYSCTL_INT(_vm_reserv, OID_AUTO, cnt_contention, CTLFLAG_RD,
+//     &cnt_contention, 0, "Pages compete for page faults");
+SYSCTL_INT(_vm_reserv, OID_AUTO, numofdeadbeef, CTLFLAG_RD,
+    &numofdeadbeef, 0, "reservations need to migrate");
+
+/* tunable parameters for asyncpromo */
+static int enable_prezero = 0;
+SYSCTL_INT(_vm_reserv, OID_AUTO, enable_prezero, CTLFLAG_RWTUN,
+    &enable_prezero, 0, "enable prezeroing pages in reservation");
+
+static int enable_compact = 0;
+SYSCTL_INT(_vm_reserv, OID_AUTO, enable_compact, CTLFLAG_RWTUN,
+    &enable_compact, 0, "enable evicting pages from inactive reservations");
+
+static int enable_sleep = 1;
+SYSCTL_INT(_vm_reserv, OID_AUTO, enable_sleep, CTLFLAG_RWTUN,
+    &enable_sleep, 0, "enable asyncpromo daemon to sleep");
+
+static int wakeup_frequency = 1;
+SYSCTL_INT(_vm_reserv, OID_AUTO, wakeup_frequency, CTLFLAG_RWTUN,
+    &wakeup_frequency, 0, "asyncpromo wakeup frequency");
+
+static int wakeup_time = 1;
+SYSCTL_INT(_vm_reserv, OID_AUTO, wakeup_time, CTLFLAG_RWTUN,
+    &wakeup_time, 0, "asyncpromo wakeup time");
+
+static int verbose = 0;
+SYSCTL_INT(_vm_reserv, OID_AUTO, verbose, CTLFLAG_RWTUN,
+    &verbose, 0, "asyncpromo verbose");
+
+static int pop_budget = 2;
+SYSCTL_INT(_vm_reserv, OID_AUTO, pop_budget, CTLFLAG_RWTUN,
+    &pop_budget, 0, "asyncpromo pre-population budget");
+
+/* 
+ * The actual threshold is 64, 
+ * because the 64th page fault will create a sp 
+ */
+static int sync_popthreshold = 31;
+SYSCTL_INT(_vm_reserv, OID_AUTO, sync_popthreshold, CTLFLAG_RWTUN,
+    &sync_popthreshold, 0, "sync promotion pop threshold");
+
+static int pop_threshold = 63;
+SYSCTL_INT(_vm_reserv, OID_AUTO, pop_threshold, CTLFLAG_RWTUN,
+    &pop_threshold, 0, "asyncpromo pop-x threshold");
+
+static int zero_budget = 512;
+SYSCTL_INT(_vm_reserv, OID_AUTO, zero_budget, CTLFLAG_RWTUN,
+    &zero_budget, 0, "asyncpromo page zero budget");
+
+static int pop_succ = 0, pop_fail = 0, pop_broken = 0;
+SYSCTL_INT(_vm_reserv, OID_AUTO, pop_succ, CTLFLAG_RWTUN,
+    &pop_succ, 0, "asyncpromo pop succ");
+SYSCTL_INT(_vm_reserv, OID_AUTO, pop_fail, CTLFLAG_RWTUN,
+    &pop_fail, 0, "asyncpromo pop fail");
+SYSCTL_INT(_vm_reserv, OID_AUTO, pop_broken, CTLFLAG_RWTUN,
+    &pop_broken, 0, "asyncpromo pop broken");;
+
+			
+#ifdef DEBUG_ASYNCPROMO
+/* debugging global variable and functions */
+vm_reserv_t rv_to_prepopulate = NULL; 
+int rv_popidx_to_prepopulate = 0;
+
+vm_page_t vm_reserv_get_page(vm_reserv_t rv, int i)
+{
+	return &rv->pages[i];
+}
+
+int vm_reserv_get_popcnt(vm_reserv_t rv)
+{
+	return rv->popcnt;
+}
+
+int vm_reserv_get_popind(vm_reserv_t rv, int i)
+{
+	return (rv->popmap[i / NBPOPMAP] & (1UL << (i % NBPOPMAP)));
+}
+
+vm_object_t vm_reserv_get_object(vm_reserv_t rv)
+{
+	return rv->object;
+}
+
+vm_pindex_t vm_reserv_get_pindex(vm_reserv_t rv)
+{
+	return rv->pindex;
+}
+
+void vm_reserv_show_all_pindex_from_page(vm_page_t m)
+{
+
+	vm_reserv_t rv = &vm_reserv_array[VM_PAGE_TO_PHYS(m) >> VM_LEVEL_0_SHIFT];
+	for(int i = 0; i < VM_LEVEL_0_NPAGES; i ++)
+		if(popmap_is_set(rv->popmap, i))
+			printf("%lu ", rv->pages[i].pindex);
+	printf("\n");
+}
+
+int vm_reserv_get_popcnt_from_page(vm_page_t m)
+{
+	vm_reserv_t rv = &vm_reserv_array[VM_PAGE_TO_PHYS(m) >> VM_LEVEL_0_SHIFT];
+	return rv->popcnt;
+}
+#endif
+
 static long vm_reserv_broken;
 SYSCTL_LONG(_vm_reserv, OID_AUTO, broken, CTLFLAG_RD,
     &vm_reserv_broken, 0, "Cumulative number of broken reservations");
@@ -218,6 +383,11 @@ static long vm_reserv_freed;
 SYSCTL_LONG(_vm_reserv, OID_AUTO, freed, CTLFLAG_RD,
     &vm_reserv_freed, 0, "Cumulative number of freed reservations");
 
+static int sysctl_vm_reserv_freesp(SYSCTL_HANDLER_ARGS);
+
+SYSCTL_PROC(_vm_reserv, OID_AUTO, freesp, CTLTYPE_INT | CTLFLAG_RD, NULL, 0,
+    sysctl_vm_reserv_freesp, "I", "Current number of available free reservations");
+
 static int sysctl_vm_reserv_fullpop(SYSCTL_HANDLER_ARGS);
 
 SYSCTL_PROC(_vm_reserv, OID_AUTO, fullpop, CTLTYPE_INT | CTLFLAG_RD, NULL, 0,
@@ -228,6 +398,16 @@ static int sysctl_vm_reserv_partpopq(SYSCTL_HANDLER_ARGS);
 SYSCTL_OID(_vm_reserv, OID_AUTO, partpopq, CTLTYPE_STRING | CTLFLAG_RD, NULL, 0,
     sysctl_vm_reserv_partpopq, "A", "Partially populated reservation queues");
 
+static int sysctl_vm_reserv_popcdf(SYSCTL_HANDLER_ARGS);
+
+SYSCTL_OID(_vm_reserv, OID_AUTO, popcdf, CTLTYPE_STRING | CTLFLAG_RD, NULL, 0,
+    sysctl_vm_reserv_popcdf, "A", "Population CDF of existing reservations");
+
+static int sysctl_vm_reserv_need_migrate(SYSCTL_HANDLER_ARGS);
+
+SYSCTL_OID(_vm_reserv, OID_AUTO, need_migrate, CTLTYPE_STRING | CTLFLAG_RD, NULL, 0,
+    sysctl_vm_reserv_need_migrate, "A", "Reservations who need page migration");
+
 static long vm_reserv_reclaimed;
 SYSCTL_LONG(_vm_reserv, OID_AUTO, reclaimed, CTLFLAG_RD,
     &vm_reserv_reclaimed, 0, "Cumulative number of reclaimed reservations");
@@ -240,6 +420,15 @@ static boolean_t	vm_reserv_has_pindex(vm_reserv_t rv,
 static void		vm_reserv_populate(vm_reserv_t rv, int index);
 static void		vm_reserv_reclaim(vm_reserv_t rv);
 
+/*
+ * Returns the number of free physical superpages
+ */
+static int
+sysctl_vm_reserv_freesp(SYSCTL_HANDLER_ARGS)
+{
+	int freesp = vm_phys_count_order_9();
+	return (sysctl_handle_int(oidp, &freesp, 0, req));
+}
 /*
  * Returns the current number of full reservations.
  *
@@ -287,6 +476,10 @@ sysctl_vm_reserv_partpopq(SYSCTL_HANDLER_ARGS)
 		unused_pages = 0;
 		mtx_lock(&vm_page_queue_free_mtx);
 		TAILQ_FOREACH(rv, &vm_rvq_partpop/*[level]*/, partpopq) {
+			/* clean side effect of using async_marker */
+			if(rv->inpartpopq & RV_MARKER)
+				continue;
+
 			counter++;
 			unused_pages += VM_LEVEL_0_NPAGES - rv->popcnt;
 		}
@@ -299,6 +492,95 @@ sysctl_vm_reserv_partpopq(SYSCTL_HANDLER_ARGS)
 	return (error);
 }
 
+/*
+ * Describes the cdf of existing reservations
+ */
+static int
+sysctl_vm_reserv_popcdf(SYSCTL_HANDLER_ARGS)
+{
+	struct sbuf sbuf;
+	vm_reserv_t rv;
+	vm_paddr_t paddr;
+	struct vm_phys_seg *seg;
+	int fullpop, segind;
+	int error, level, i;
+	int cdf[513];
+
+	error = sysctl_wire_old_buffer(req, 0);
+	if (error != 0)
+		return (error);
+	sbuf_new_for_sysctl(&sbuf, NULL, 4096, req);
+	sbuf_printf(&sbuf, "\n");
+
+	for(i = 0; i < 512; i ++)
+		cdf[i] = 0;
+	for (level = -1; level <= VM_NRESERVLEVEL - 2; level++) {
+		mtx_lock(&vm_page_queue_free_mtx);
+		TAILQ_FOREACH(rv, &vm_rvq_partpop/*[level]*/, partpopq) {
+			/* clean side effect of using rv_marker */
+			if(rv->inpartpopq & RV_MARKER)
+				continue;
+			cdf[rv->popcnt] ++;
+		}
+		mtx_unlock(&vm_page_queue_free_mtx);
+		// sbuf_printf(&sbuf, "%5d: %6dK, %6d\n", level,
+		//     unused_pages * ((int)PAGE_SIZE / 1024), counter);
+	}
+
+
+	fullpop = 0;
+	for (segind = 0; segind < vm_phys_nsegs; segind++) {
+		seg = &vm_phys_segs[segind];
+		paddr = roundup2(seg->start, VM_LEVEL_0_SIZE);
+		while (paddr + VM_LEVEL_0_SIZE <= seg->end) {
+			rv = &vm_reserv_array[paddr >> VM_LEVEL_0_SHIFT];
+			fullpop += rv->popcnt == VM_LEVEL_0_NPAGES;
+			paddr += VM_LEVEL_0_SIZE;
+		}
+	}
+	cdf[512] = fullpop;
+	for(i = 1; i < 513; i ++)
+		sbuf_printf(&sbuf, "%d ", cdf[i]);
+	sbuf_printf(&sbuf, "\n");
+
+	error = sbuf_finish(&sbuf);
+	sbuf_delete(&sbuf);
+	return (error);
+}
+
+/*
+ * Describes the current state of the partially populated reservation queue.
+ */
+static int
+sysctl_vm_reserv_need_migrate(SYSCTL_HANDLER_ARGS)
+{
+	struct sbuf sbuf;
+	vm_reserv_t rv;
+	int counter, error, level;
+
+	error = sysctl_wire_old_buffer(req, 0);
+	if (error != 0)
+		return (error);
+	sbuf_new_for_sysctl(&sbuf, NULL, 128, req);
+	sbuf_printf(&sbuf, "\nLEVEL     NUMBER\n\n");
+	for (level = -1; level <= VM_NRESERVLEVEL - 2; level++) {
+		counter = 0;
+		mtx_lock(&vm_page_queue_free_mtx);
+		TAILQ_FOREACH(rv, &vm_rvq_partpop/*[level]*/, partpopq) {
+			/* clean side effect of using rv_marker */
+			if(rv->inpartpopq & RV_MARKER)
+				continue;
+			if(rv->inpartpopq & RV_NEEDMIGRATE)
+				counter++;
+		}
+		mtx_unlock(&vm_page_queue_free_mtx);
+		sbuf_printf(&sbuf, "%5d: %6d\n", level, counter);
+	}
+	error = sbuf_finish(&sbuf);
+	sbuf_delete(&sbuf);
+	return (error);
+}
+
 /*
  * Reduces the given reservation's population count.  If the population count
  * becomes zero, the reservation is destroyed.  Additionally, moves the
@@ -319,9 +601,9 @@ vm_reserv_depopulate(vm_reserv_t rv, int index)
 	    index));
 	KASSERT(rv->popcnt > 0,
 	    ("vm_reserv_depopulate: reserv %p's popcnt is corrupted", rv));
-	if (rv->inpartpopq) {
+	if (is_inpartpopq(rv->inpartpopq)) {
 		TAILQ_REMOVE(&vm_rvq_partpop, rv, partpopq);
-		rv->inpartpopq = FALSE;
+		clear_inpartpopq(rv->inpartpopq);
 	} else {
 		KASSERT(rv->pages->psind == 1,
 		    ("vm_reserv_depopulate: reserv %p is already demoted",
@@ -332,11 +614,14 @@ vm_reserv_depopulate(vm_reserv_t rv, int index)
 	rv->popcnt--;
 	if (rv->popcnt == 0) {
 		LIST_REMOVE(rv, objq);
+		clear_all(rv->inpartpopq);
 		rv->object = NULL;
 		vm_phys_free_pages(rv->pages, VM_LEVEL_0_ORDER);
 		vm_reserv_freed++;
 	} else {
-		rv->inpartpopq = TRUE;
+		/* maybe prepopulation should skip badboy */
+		set_inpartpopq(rv->inpartpopq);
+		rv->timestamp = ticks;
 		TAILQ_INSERT_TAIL(&vm_rvq_partpop, rv, partpopq);
 	}
 }
@@ -362,6 +647,91 @@ vm_reserv_has_pindex(vm_reserv_t rv, vm_pindex_t pindex)
 	return (((pindex - rv->pindex) & ~(VM_LEVEL_0_NPAGES - 1)) == 0);
 }
 
+/* [syncpromo]
+	functions to help vm_fault syncronously promote superpages
+ */
+bool
+vm_reserv_satisfy_sync_promotion(vm_page_t m)
+{
+	vm_reserv_t rv = &vm_reserv_array[VM_PAGE_TO_PHYS(m) >> VM_LEVEL_0_SHIFT];
+	/* expect m to be installed in a reservation */
+	if(rv->object != NULL && rv->object == m->object 
+		&& m->pindex >= rv->pindex
+		&& m->pindex < rv->pindex + VM_LEVEL_0_NPAGES
+		&& popmap_is_set(rv->popmap, m->pindex - rv->pindex) 
+		&& rv->inpartpopq == RV_INPARTPOPQ
+		&& rv->popcnt >= sync_popthreshold)
+		return TRUE;
+	else
+		return FALSE;
+} 
+
+bool
+vm_reserv_satisfy_adj_promotion(vm_page_t m)
+{
+	vm_reserv_t rv = &vm_reserv_array[VM_PAGE_TO_PHYS(m) >> VM_LEVEL_0_SHIFT];
+	/* expect m to be installed in a reservation */
+	if(rv->object != NULL && rv->object == m->object 
+		&& m->pindex >= rv->pindex
+		&& m->pindex < rv->pindex + VM_LEVEL_0_NPAGES
+		&& popmap_is_set(rv->popmap, m->pindex - rv->pindex) 
+		&& rv->inpartpopq == RV_INPARTPOPQ)
+		return TRUE;
+	else
+		return FALSE;
+} 
+
+vm_pindex_t
+vm_reserv_pindex_from_page(vm_page_t m)
+{
+	return vm_reserv_array[VM_PAGE_TO_PHYS(m) >> VM_LEVEL_0_SHIFT].pindex;
+}
+
+void
+vm_reserv_copy_popmap_from_page(vm_page_t m, popmap_t *popmap)
+{
+	int i;
+	for(i = 0; i < NPOPMAP; i ++)
+		popmap[i] = vm_reserv_array[VM_PAGE_TO_PHYS(m) >> VM_LEVEL_0_SHIFT].popmap[i];
+}
+
+int
+vm_reserv_popmap_is_clear(vm_page_t m, int i)
+{
+	return popmap_is_clear(vm_reserv_array[VM_PAGE_TO_PHYS(m) >> VM_LEVEL_0_SHIFT].popmap, i);
+}
+
+int
+vm_reserv_get_next_set_index(vm_page_t m, int i)
+{
+	vm_reserv_t rv = &vm_reserv_array[VM_PAGE_TO_PHYS(m) >> VM_LEVEL_0_SHIFT];
+	while(popmap_is_clear(rv->popmap, i) && i < 512)
+		i ++;
+	return i;
+}
+
+int
+vm_reserv_get_next_clear_index(vm_page_t m, int i)
+{
+	vm_reserv_t rv = &vm_reserv_array[VM_PAGE_TO_PHYS(m) >> VM_LEVEL_0_SHIFT];
+	while(popmap_is_set(rv->popmap, i) && i < 512)
+		i ++;
+	return i;
+}
+
+void
+vm_reserv_mark_bad(vm_page_t m)
+{
+	vm_reserv_array[VM_PAGE_TO_PHYS(m) >> VM_LEVEL_0_SHIFT].inpartpopq |= RV_BADBOY;
+	return ;
+}
+
+bool
+vm_reserv_is_full(vm_page_t m)
+{
+	return (vm_reserv_array[VM_PAGE_TO_PHYS(m) >> VM_LEVEL_0_SHIFT].popcnt == VM_LEVEL_0_NPAGES);
+}
+
 /*
  * Increases the given reservation's population count.  Moves the reservation
  * to the tail of the partially populated reservation queue.
@@ -382,14 +752,15 @@ vm_reserv_populate(vm_reserv_t rv, int index)
 	    ("vm_reserv_populate: reserv %p is already full", rv));
 	KASSERT(rv->pages->psind == 0,
 	    ("vm_reserv_populate: reserv %p is already promoted", rv));
-	if (rv->inpartpopq) {
+	if (is_inpartpopq(rv->inpartpopq)) {
 		TAILQ_REMOVE(&vm_rvq_partpop, rv, partpopq);
-		rv->inpartpopq = FALSE;
+		clear_inpartpopq(rv->inpartpopq);
 	}
 	popmap_set(rv->popmap, index);
 	rv->popcnt++;
 	if (rv->popcnt < VM_LEVEL_0_NPAGES) {
-		rv->inpartpopq = TRUE;
+		set_inpartpopq(rv->inpartpopq);
+		rv->timestamp = ticks;
 		TAILQ_INSERT_TAIL(&vm_rvq_partpop, rv, partpopq);
 	} else
 		rv->pages->psind = 1;
@@ -559,7 +930,7 @@ vm_reserv_alloc_contig(vm_object_t object, vm_pindex_t pindex, u_long npages,
 		KASSERT(rv->popcnt == 0,
 		    ("vm_reserv_alloc_contig: reserv %p's popcnt is corrupted",
 		    rv));
-		KASSERT(!rv->inpartpopq,
+		KASSERT(!is_inpartpopq(rv->inpartpopq),
 		    ("vm_reserv_alloc_contig: reserv %p's inpartpopq is TRUE",
 		    rv));
 		for (i = 0; i < NPOPMAP; i++)
@@ -703,7 +1074,7 @@ vm_reserv_alloc_page(vm_object_t object, vm_pindex_t pindex, vm_page_t mpred)
 	rv->pindex = first;
 	KASSERT(rv->popcnt == 0,
 	    ("vm_reserv_alloc_page: reserv %p's popcnt is corrupted", rv));
-	KASSERT(!rv->inpartpopq,
+	KASSERT(!is_inpartpopq(rv->inpartpopq),
 	    ("vm_reserv_alloc_page: reserv %p's inpartpopq is TRUE", rv));
 	for (i = 0; i < NPOPMAP; i++)
 		KASSERT(rv->popmap[i] == 0,
@@ -742,7 +1113,7 @@ vm_reserv_break(vm_reserv_t rv)
 	mtx_assert(&vm_page_queue_free_mtx, MA_OWNED);
 	KASSERT(rv->object != NULL,
 	    ("vm_reserv_break: reserv %p is free", rv));
-	KASSERT(!rv->inpartpopq,
+	KASSERT(!is_inpartpopq(rv->inpartpopq),
 	    ("vm_reserv_break: reserv %p's inpartpopq is TRUE", rv));
 	LIST_REMOVE(rv, objq);
 	rv->object = NULL;
@@ -803,10 +1174,10 @@ vm_reserv_break_all(vm_object_t object)
 	while ((rv = LIST_FIRST(&object->rvq)) != NULL) {
 		KASSERT(rv->object == object,
 		    ("vm_reserv_break_all: reserv %p is corrupted", rv));
-		if (rv->inpartpopq) {
+		if (is_inpartpopq(rv->inpartpopq)) {
 			TAILQ_REMOVE(&vm_rvq_partpop, rv, partpopq);
-			rv->inpartpopq = FALSE;
 		}
+		clear_all(rv->inpartpopq);
 		vm_reserv_break(rv);
 	}
 	mtx_unlock(&vm_page_queue_free_mtx);
@@ -912,10 +1283,10 @@ vm_reserv_reclaim(vm_reserv_t rv)
 {
 
 	mtx_assert(&vm_page_queue_free_mtx, MA_OWNED);
-	KASSERT(rv->inpartpopq,
+	KASSERT(is_inpartpopq(rv->inpartpopq),
 	    ("vm_reserv_reclaim: reserv %p's inpartpopq is FALSE", rv));
 	TAILQ_REMOVE(&vm_rvq_partpop, rv, partpopq);
-	rv->inpartpopq = FALSE;
+	clear_all(rv->inpartpopq);
 	vm_reserv_break(rv);
 	vm_reserv_reclaimed++;
 }
@@ -933,7 +1304,10 @@ vm_reserv_reclaim_inactive(void)
 	vm_reserv_t rv;
 
 	mtx_assert(&vm_page_queue_free_mtx, MA_OWNED);
-	if ((rv = TAILQ_FIRST(&vm_rvq_partpop)) != NULL) {
+	rv = TAILQ_FIRST(&vm_rvq_partpop);
+	if(rv->inpartpopq & RV_MARKER)
+		rv = TAILQ_NEXT(rv, partpopq);
+	if (rv != NULL) {
 		vm_reserv_reclaim(rv);
 		return (TRUE);
 	}
@@ -961,6 +1335,10 @@ vm_reserv_reclaim_contig(u_long npages, vm_paddr_t low, vm_paddr_t high,
 		return (FALSE);
 	size = npages << PAGE_SHIFT;
 	TAILQ_FOREACH(rv, &vm_rvq_partpop, partpopq) {
+		/* clean side effect of using aync_marker */
+		if(rv->inpartpopq & RV_MARKER)
+			continue;
+
 		pa = VM_PAGE_TO_PHYS(&rv->pages[VM_LEVEL_0_NPAGES - 1]);
 		if (pa + PAGE_SIZE - size < low) {
 			/* This entire reservation is too low; go to next. */
@@ -1042,7 +1420,8 @@ void
 vm_reserv_rename(vm_page_t m, vm_object_t new_object, vm_object_t old_object,
     vm_pindex_t old_object_offset)
 {
-	vm_reserv_t rv;
+	vm_reserv_t rv, rv_tmp;
+	boolean_t merge;
 
 	VM_OBJECT_ASSERT_WLOCKED(new_object);
 	rv = vm_reserv_from_page(m);
@@ -1053,6 +1432,34 @@ vm_reserv_rename(vm_page_t m, vm_object_t new_object, vm_object_t old_object,
 			LIST_INSERT_HEAD(&new_object->rvq, rv, objq);
 			rv->object = new_object;
 			rv->pindex -= old_object_offset;
+
+			/*
+			 * [asyncpromo] Check pindex collision in &new_object->rvq
+			 * Not sure if it is worth, as vm_page_queue_free_mtx is locked
+			 */
+			merge = FALSE;
+			rv_tmp = LIST_NEXT(rv, objq);
+			if(rv_tmp != NULL)
+				/* scan the rest of objq from 2nd element, if size >= 2 */
+				LIST_FOREACH_FROM(rv_tmp, &new_object->rvq, objq)
+					if(rv_tmp->pindex == rv->pindex)
+					{
+						set_migrate(rv_tmp->inpartpopq);
+						merge = TRUE;
+					}
+			if(merge)
+				set_migrate(rv->inpartpopq);
+			// deadbeef_scan ++;
+
+			/* 
+			 * [asyncpromo]
+			 * lazy mechanism:
+			 * mark the reservation as "transferred", instead of detecting
+			 * a pindex collision.
+			 * A transferred reservation has a small possibility to share
+			 * its pindex with another one in the 
+			 */
+			// rv->inpartpopq |= RV_TRANSFERRED;
 		}
 		mtx_unlock(&vm_page_queue_free_mtx);
 	}
@@ -1123,4 +1530,595 @@ vm_reserv_to_superpage(vm_page_t m)
 	    rv->pages : NULL);
 }
 
+/*
+ * Below is the code for async_promote daemon
+ * The daemon should periodically scan partpopq to determine:
+ * a. A reservation should be promoted
+ * b. use temporal stores to zero reservations and promote as superpage
+ */
+
+// static int idlezero_enable_default = 0;
+/* Defer setting the enable flag until the kthread is running. */
+// static int idlezero_enable = 0;
+// SYSCTL_INT(_vm, OID_AUTO, idlezero_enable, CTLFLAG_RWTUN, &idlezero_enable, 0,
+//     "Allow the kernel to use idle cpu cycles to zero-out pages");
+/*
+ * Implement the asynchronous early-promotion mechanism.
+ */
+// static boolean_t wakeup_needed = FALSE;
+static int async_promote;
+
+/* 
+ * Check if rv can never get fully populated.
+ * If a shadow object collapse, reservation could be renamed and merged into the private object,
+ * where there could be existing reservation sharing the same pindex
+ * The free page queue lock must be acquired, so do not use this function now
+ */
+#ifdef DEBUG_ASYNCPROMO
+boolean_t
+#else
+static boolean_t
+#endif
+vm_reserv_is_deadbeef(vm_reserv_t rv)
+{
+	vm_object_t obj;
+	vm_pindex_t pindex;
+	vm_reserv_t rvi;
+
+	// VM_OBJECT_ASSERT_RLOCKED(rv->object);
+
+	obj = rv->object;
+	pindex = rv->pindex;
+	LIST_FOREACH(rvi, &obj->rvq, objq)
+	{
+		if(rvi != rv && rvi->pindex == pindex)
+			return TRUE;
+	}
+	return FALSE;
+}
+
+/* 
+ * This function pre-populate and zero all remaining free pages in a reservation
+ * Subsequent page fault will use pmap_enter to create a superpage directly
+ * This function does not necesssarily succeed. The free page queue lock is
+ * held only inside the vm_page_alloc when allocating a page from the reservation
+ * For performance, it is much better not to hold the free page queue lock,
+ * so, the vm_page_alloc may need to fail and detect if the reservation is broken,
+ * and we cannot assume the reservation's object is going to be valid all the time.
+ * The function would return whether prepopulate succeeds
+ * Caution: rv can be broken at any time without holding free page queue lock
+ * Return indicates whether the pre-population succeeds or not
+
+ * If rv exists then populate and zero all pages in the reservation
+ * This solution uses vm_page_alloc() to directly install all free pages
+ * and then zero all these busy pages and release their busy lock.
+ * After that, a cheap page fault on any page would promote this reservation as
+ * a superpage.
+ * 
+ * Possible Optimization:
+ * Delete one page mapping to enforce a soft page fault to install a superpage
+ * mapping.
+ * 
+ * Another solution:
+ * If you want to create all superpages, dig into all pmaps and install superpages
+*/
+static boolean_t
+vm_reserv_prepopulate(vm_reserv_t rv)
+{
+	int i;
+	vm_page_t m, mpred;
+	vm_object_t object;
+
+	mtx_assert(&vm_page_queue_free_mtx, MA_NOTOWNED);
+	/* [race] rv may be broken here */
+	if((object = rv->object) != NULL)
+		VM_OBJECT_WLOCK(object);
+	else
+	{
+		// printf("[prepopulate] FAIL: reservation gets broken\n");
+
+		// printf("[prepopulate] FAIL_1: succ|brok|fail|zero:[%d|%d|%d|%d]\n", 
+		// 	pop_succ, pop_broken, pop_fail, async_prezero);
+		pop_broken ++;
+		return FALSE;
+	}
+
+#ifdef DEBUG_ASYNCPROMO
+	rv_to_prepopulate = rv;
+#endif
+
+	mpred = vm_page_find_most(object, rv->pindex + VM_LEVEL_0_NPAGES - 1);
+	/* maybe I should simply scan backwards 
+	maintain a vm_page_t cursor which is the last page I added
+	with TAILQ_NEXT at the page to look at the pindex of the next page
+	if it is equal to the index I am trying to prepopulate
+	*/
+	for(i = VM_LEVEL_0_NPAGES - 1; i >= 0; i --)
+	{
+		/* rv can be broken here, because free page queue is not locked */
+		if(rv->object == NULL) 
+			goto fail;	
+
+		if(mpred != NULL && mpred->pindex == rv->pindex + i)
+		{
+			/* only examine consistency when mpred is reached, otherwise it is safe to prepopulate */
+			if(popmap_is_set(rv->popmap, i))
+			{			
+				if(mpred == &rv->pages[i])
+					mpred = TAILQ_PREV(mpred, pglist, listq);
+				else
+					panic("%s: inconsistency found at pindex %lu: object holds 2 pages for the same pindex",
+					    __func__, rv->pindex + i);
+			}
+			else
+				/* mpred should have been populated inside the reservation. Let's abort now */
+				goto fail;
+		}
+		else
+		if(popmap_is_clear(rv->popmap, i))
+		{
+			/* prepopulate this page */
+#ifdef DEBUG_ASYNCPROMO
+			rv_popidx_to_prepopulate = i;
+#endif	
+			/* 
+			 * requires vm_page_alloc to allocate a free page from a reservation 
+			 * The vm_page_alloc has been modified such that with VM_ALLOC_RESERVONLY,
+			 * it must fail if it cannot allocate a page from a reservation
+			 */
+
+			/* [potential bug]:
+				When swapping is triggered, reservations may be
+				broken and paged in again as free pages,
+				reservation may not be populated because of 
+				fragmentation.
+				We currently ignore this bug by not working on any
+				benchmark requiring swapping
+			 */
+			// if(vm_page_lookup(object, rv->pindex + i))
+				/* such page exist, fail prepopulating this reservation */
+				// goto fail;
+			m = vm_page_alloc(object, rv->pindex + i, 
+				VM_ALLOC_NORMAL | VM_ALLOC_ZERO | VM_ALLOC_RESERVONLY);
+			/*
+			 * TODO: get tail_next and double check and see if there is
+			 any inconsistency, track how many aborts there are */
+			/* 
+			 * If the allocation fails, the reservation is already broken, 
+			 * do not pre-zero it
+			 */
+			if(m == NULL)
+				goto fail;
+			/* release the object lock to do pre-zero */
+			VM_OBJECT_WUNLOCK(object);
+			/* check if page m is correctly allocated and xbusied */
+			vm_page_assert_xbusied(m);
+			/* m is xbusied after vm_page_alloc, so we can safely zero it */
+			if((m->flags & PG_ZERO) == 0)
+			{
+				/* 
+				 * pages allocated in the reservation are not in freelist,
+				 * but acquiring free page queue list lock can prevent a page fault zeroing this page
+				 * So it is safe here to zero this page.
+				 * we cannot unlock the page queue lock to free the page, because it is not removed
+				 * from a reservation pool
+				 */
+				pmap_zero_page_idle(m);
+				m->flags |= PG_ZERO;
+				vm_page_zero_count ++;
+				async_prezero++;
+			}
+			else
+				async_skipzero ++;
+			/* lock the vm object again once page zeroing is done */
+			VM_OBJECT_WLOCK(object);
+
+			/* the lock contention optimization of vm_object could end up with a NULL
+			 * object that has been released when we zero the page.
+			 * only validate and enqueue the page if the object is still alive
+			 */
+			if(m->object != NULL && m->object == rv->object)
+			{	
+				/* validate, explicitly dirty and unbusy this page (copied from vm_page_grab_pages) */
+				m->valid = VM_PAGE_BITS_ALL;
+				/* put it in the active queue */
+				vm_page_lock(m);
+				vm_page_activate(m);
+				vm_page_unlock(m);
+				vm_page_xunbusy(m);
+			}
+			else
+			{
+				// panic("%s: strange vm object detected",
+				//     __func__);
+				/* unbusy it anyway */
+				vm_page_xunbusy(m);
+				goto fail;
+			}
+
+		}
+		// else
+		// 	 do nothing, the page has just been faulted, you are in lock contention 
+		// 	;
+	}
+
+	/* 
+	 * The prepopulation succeeds 
+	 * The reservation must get fully populated now
+	 */
+	VM_OBJECT_WUNLOCK(object);	
+	if(verbose && ((pop_succ % 10) == 0))
+		printf("[prepopulate] succ|brok|fail|zero:[%d|%d|%d|%d]\n", 
+			pop_succ, pop_broken, pop_fail, async_prezero);
+	// printf("[prepopulate] SUCC: %d pages allocated, %d pages zeroed, %d zeroed in total\n", 
+	// 	already_zeroed + counter, counter, async_prezero);
+	// if(need_to_zero != already_zeroed + counter)
+	// {
+	// 	 The difference is from lock contention,
+	// 		page fault is competing for the pages
+		 
+	// 	cnt_contention += need_to_zero - already_zeroed - counter;
+	// }
+	pop_succ ++;
+	return TRUE;
+
+fail:
+	VM_OBJECT_WUNLOCK(object);
+	if(rv->object == NULL)
+	{
+		// printf("[prepopulate] FAIL: rv gets broken\n");
+		// printf("[prepopulate] FAIL_2: succ|brok|fail|zero:[%d|%d|%d|%d]\n", 
+		// 	pop_succ, pop_broken, pop_fail, async_prezero);
+		pop_broken ++;
+	}
+	else
+	{
+		// printf("[prepopulate] FAIL: fails to alloc a page, rv is marked as badboy\n");
+		// printf("[prepopulate] FAIL_3: succ|brok|fail|zero:[%d|%d|%d|%d]\n", 
+		// 	pop_succ, pop_broken, pop_fail, async_prezero);
+		// printf("[debug] OBJECT: flags[%x]\n", object->flags);
+		rv->inpartpopq |= RV_BADBOY;
+		pop_fail ++;
+	}
+	return FALSE;
+}
+
+/* 
+ * scan partpopq of reservations for async promotion 
+ * This function is serving for asyncpromo daemon, it is called
+ * with vm_page_queue_free_mtx locked 
+ */
+static void
+vm_reserv_scan_partpopq(void)
+{
+	vm_reserv_t rv, prev;
+	vm_object_t obj;
+	int counter, eligible, pop_cnt; 
+	boolean_t queue_locked;
+
+	/*  This code segment is hardcoded for 2MB superpages */
+	counter = eligible = pop_cnt = 0;
+
+	/* 
+	 *  The traversal uses an async_marker to help locate the position in partpopq
+	 *  So that even though vm_page_queue_free_mtx is not always held, 
+	 *  it is safe to keep traversing the TAILQ
+	 */
+	mtx_lock(&vm_page_queue_free_mtx);
+	// queue_locked = TRUE;
+	for (rv = TAILQ_LAST(&vm_rvq_partpop, rvlist);
+		 rv != NULL && pop_cnt < pop_budget;
+		 rv = prev) {
+		mtx_assert(&vm_page_queue_free_mtx, MA_OWNED);
+		/* there are async and compact markers */
+		if(rv->inpartpopq & RV_MARKER)
+			continue;
+		// prev = TAILQ_PREV(rv, rvlist, partpopq);
+		TAILQ_INSERT_BEFORE(rv, &async_marker, partpopq);
+		mtx_unlock(&vm_page_queue_free_mtx);
+		queue_locked = FALSE;
+
+		counter ++;
+		obj = rv->object;
+		/*
+		 * Find the reservation possible and eligible to be promoted.
+		 * A deadbeef reservation can never be promoted, 
+		 * because there are other reservations
+		 * transferred to the same object with the same pindex. 
+		 * None of them can be fully populated.
+		 */
+		if(obj != NULL && (obj->type == OBJT_DEFAULT || obj->type == OBJT_SWAP)
+			&& obj->backing_object == NULL // && obj->handle == NULL
+			&& rv->inpartpopq == RV_INPARTPOPQ //!vm_reserv_is_deadbeef(rv)
+			&& rv->popcnt >= pop_threshold)
+		{
+			// printf("[scan partpopq] going to prepopulate...\n");
+			/* rv will be removed from partpopq if gets prepopulated */
+			pop_cnt += vm_reserv_prepopulate(rv);
+		}
+
+		if (!queue_locked) {
+			mtx_lock(&vm_page_queue_free_mtx);
+			queue_locked = TRUE;
+		}
+		mtx_assert(&vm_page_queue_free_mtx, MA_OWNED);
+		prev = TAILQ_PREV(&async_marker, rvlist, partpopq);
+		TAILQ_REMOVE(&vm_rvq_partpop, &async_marker, partpopq);
+	}
+	mtx_unlock(&vm_page_queue_free_mtx);
+
+	// printf("[scan partpopq] scanned: %d, pre-populated: %d\n",
+	// 	counter, pop_cnt);
+	return ;
+}
+
+/* inactive reservation threshold is 10s, migration budget is 10MB/s */
+static int inactive_thre = 10000, migrate_budget = 10 * 256;
+
+SYSCTL_INT(_vm_reserv, OID_AUTO, inactive_thre, CTLFLAG_RWTUN,
+    &inactive_thre, 0, "inactive timeout threshold");
+SYSCTL_INT(_vm_reserv, OID_AUTO, migrate_budget, CTLFLAG_RWTUN,
+    &migrate_budget, 0, "memory compaction budget");
+
+static void
+vm_reserv_evict_inactive(void)
+{
+	int work, exit;
+	vm_reserv_t next, rv;
+	vm_object_t obj;
+
+	work = exit = 0;
+	// current_tick = ticks;
+	// printf("compaction start time: %d\n", current_tick);
+	/* 
+	 *  The traversal uses evict_marker to help locate the position in partpopq
+	 *  So that even though vm_page_queue_free_mtx is not always held, 
+	 *  it is safe to keep traversing the TAILQ
+	 */
+	mtx_lock(&vm_page_queue_free_mtx);
+	for (rv = TAILQ_FIRST(&vm_rvq_partpop);
+		 rv != NULL;
+		 rv = next) 
+	{
+		mtx_assert(&vm_page_queue_free_mtx, MA_OWNED);
+		/* there are async and evict markers */
+		if(rv->inpartpopq & RV_MARKER)
+			continue;
+
+		TAILQ_INSERT_AFTER(&vm_rvq_partpop, rv, &evict_marker, partpopq);
+		mtx_unlock(&vm_page_queue_free_mtx);
+
+		// if(is_skip(rv->inpartpopq) != 0)
+		// 	goto skip;
+
+		// printf("rv timestamp %d, %x\n", rv->timestamp, rv->inpartpopq);
+		if(work < migrate_budget &&
+			(ticks - rv->timestamp > inactive_thre
+			|| need_migrate(rv->inpartpopq)))
+		{
+			obj = rv->object;
+			if(obj != NULL && (obj->flags & (OBJ_FICTITIOUS | OBJ_UNMANAGED)) == 0)
+			{
+				/* let's evict this reservation */
+				work += rv->popcnt;
+				vm_page_reclaim_run(VM_ALLOC_NORMAL, 512, rv->pages, 0);
+			}
+		}
+		else
+			/* optimization because the list is temporally ordered */
+			exit = 1;
+
+// skip:
+		mtx_lock(&vm_page_queue_free_mtx);
+		mtx_assert(&vm_page_queue_free_mtx, MA_OWNED);
+		next = TAILQ_NEXT(&evict_marker, partpopq);
+		TAILQ_REMOVE(&vm_rvq_partpop, &evict_marker, partpopq);
+		if(exit)
+			break;
+	}
+	mtx_unlock(&vm_page_queue_free_mtx);
+	return ;
+}
+
+/*
+ * TODO: A wakeup mechanism is required so that a page fault might 
+   kick off asyncpromo
+ */
+static void
+vm_reserv_asyncpromo(void __unused *arg)
+{
+	vm_reserv_init_marker(&async_marker);
+	vm_reserv_init_marker(&evict_marker);
+
+	for (;;) 
+	{
+		/* Are there reservations satisfying promotion condition ? */
+		if (enable_prezero) 
+		{
+			vm_reserv_scan_partpopq();
+			/* 
+			 * tsleep does not unlock any mutex lock
+			 * wakeup_frequency is tunable via sysctl, default = 0.1hz
+			 * hz ~= 1s
+			 */
+		}
+
+		/* evict inactive partpopq */
+		if (enable_compact)
+		{
+			vm_reserv_evict_inactive();
+		}
+
+		/* Try to sleep no matter if enable_prezero */
+		if(enable_sleep)
+		{	
+			/* sleeps without holding any lock */
+			tsleep(&async_promote, 0,
+			    "asyncpromo", wakeup_frequency * hz / wakeup_time);
+		}
+	}
+}
+
+static void
+asyncpromo_start(void __unused *arg)
+{
+	int error;
+	struct proc *p;
+	struct thread *td;
+
+	error = kproc_create(vm_reserv_asyncpromo, NULL, &p, RFSTOPPED, 0, 
+		"asyncpromo");
+	if (error)
+		panic("asyncpromo_start: error %d\n", error);
+	td = FIRST_THREAD_IN_PROC(p);
+	thread_lock(td);
+
+	/* We're an idle task, don't count us in the load. */
+	td->td_flags |= TDF_NOLOAD;
+	sched_class(td, PRI_IDLE);
+	sched_prio(td, PRI_MAX_IDLE);
+	sched_add(td, SRQ_BORING);
+	thread_unlock(td);
+}
+SYSINIT(asyncpromo, SI_SUB_KTHREAD_VM, SI_ORDER_ANY, asyncpromo_start, NULL);
+
+
+
+static int compact_method = 0;
+SYSCTL_INT(_vm_reserv, OID_AUTO, compact_method, CTLFLAG_RWTUN,
+    &compact_method, 0, "compaction method");
+
+static void
+vm_reserv_compact()
+{
+	vm_reserv_t rv, prev;
+	vm_object_t obj;
+	vm_page_t m_run;
+	vm_paddr_t high;
+	int available_order_9;
+
+	vm_reserv_init_marker(&compact_marker);
+
+	if(compact_method)
+		uprintf("\nset high to the end of each reservation\n");
+	else
+		uprintf("\nset high to 0\n");
+
+	available_order_9 = vm_phys_count_order_9();
+	uprintf("available order-9 pages before compaction:\n%d\n", available_order_9);
+	uprintf("2MB FMFI before compaction:\n%d / 100\n", 100 - 100 * available_order_9 * (1 << 9) / vm_cnt.v_free_count);
+	/* 
+	 *  TODO: traverse the list forwards instead of backwards.
+	 */
+	mtx_lock(&vm_page_queue_free_mtx);
+	for (rv = TAILQ_LAST(&vm_rvq_partpop, rvlist);
+		 rv != NULL;
+		 rv = prev) {
+		mtx_assert(&vm_page_queue_free_mtx, MA_OWNED);
+		/* there are async and compact markers */
+		if(rv->inpartpopq & RV_MARKER)
+			continue;
+		// prev = TAILQ_PREV(rv, rvlist, partpopq);
+		TAILQ_INSERT_BEFORE(rv, &compact_marker, partpopq);
+		mtx_unlock(&vm_page_queue_free_mtx);
+
+		obj = rv->object;
+		if((obj->flags & (OBJ_FICTITIOUS | OBJ_UNMANAGED)) == 0)
+		{
+			/* let's reclaim this reservation */
+			m_run = rv->pages;
+			if(compact_method)
+				high = VM_PAGE_TO_PHYS(m_run) + NBPDR;
+			else
+				high = 0;
+
+			vm_page_reclaim_run(VM_ALLOC_NORMAL, 512, m_run, high);
+		}
+
+		mtx_lock(&vm_page_queue_free_mtx);
+		mtx_assert(&vm_page_queue_free_mtx, MA_OWNED);
+		prev = TAILQ_PREV(&compact_marker, rvlist, partpopq);
+		TAILQ_REMOVE(&vm_rvq_partpop, &compact_marker, partpopq);
+	}
+	mtx_unlock(&vm_page_queue_free_mtx);
+	available_order_9 = vm_phys_count_order_9();
+	uprintf("available order-9 pages before compaction:\n%d\n", available_order_9);
+	uprintf("2MB FMFI after compaction:\n%d / 100\n", 100 - 100 * available_order_9 * (1 << 9) / vm_cnt.v_free_count);
+}
+
+/*
+ * a debug sysctl to relocate all partial reservations
+ */
+static int
+debug_vm_reserv_compact(SYSCTL_HANDLER_ARGS)
+{
+	int error, i;
+
+	i = 0;
+	error = sysctl_handle_int(oidp, &i, 0, req);
+	if (error)
+		return (error);
+	if (i != 0)
+		vm_reserv_compact();
+	return (0);
+}
+
+SYSCTL_PROC(_vm_reserv, OID_AUTO, compact, CTLTYPE_INT | CTLFLAG_RW, 0, 0,
+    debug_vm_reserv_compact, "I", "compact all partial popq");
+
+
+static void
+vm_reserv_count_inactive()
+{
+	struct vm_domain *vmd;
+	vm_page_t m, next;
+	struct vm_pagequeue *pq;
+	int maxscan;
+
+	vmd = &vm_dom[0];
+	int clean_cnt = 0;
+
+	pq = &vmd->vmd_pagequeues[PQ_INACTIVE];
+	maxscan = pq->pq_cnt;
+	vm_pagequeue_lock(pq);
+	for (m = TAILQ_FIRST(&pq->pq_pl);
+	     m != NULL && maxscan-- > 0;
+	     m = next) {
+
+		next = TAILQ_NEXT(m, plinks.q);
+
+		/*
+		 * skip marker pages
+		 */
+		if (m->flags & PG_MARKER)
+			continue;
+
+		if (m->dirty != VM_PAGE_BITS_ALL && pmap_is_modified(m))
+			clean_cnt ++;
+	}
+	vm_pagequeue_unlock(pq);
+
+	uprintf("\nNumber of clean pages: %d\n", clean_cnt);
+}
+
+/*
+ * a debug sysctl to relocate all partial reservations
+ */
+static int
+debug_vm_reserv_count(SYSCTL_HANDLER_ARGS)
+{
+	int error, i;
+
+	i = 0;
+	error = sysctl_handle_int(oidp, &i, 0, req);
+	if (error)
+		return (error);
+	if (i != 0)
+		vm_reserv_count_inactive();
+	return (0);
+}
+
+SYSCTL_PROC(_vm_reserv, OID_AUTO, count, CTLTYPE_INT | CTLFLAG_RW, 0, 0,
+    debug_vm_reserv_count, "I", "count clean inactive pages");
+
 #endif	/* VM_NRESERVLEVEL > 0 */
diff --git a/sys/vm/vm_reserv.h b/sys/vm/vm_reserv.h
index 3d9472de1b1..cb2479d7df0 100644
--- a/sys/vm/vm_reserv.h
+++ b/sys/vm/vm_reserv.h
@@ -66,6 +66,17 @@ vm_paddr_t	vm_reserv_startup(vm_offset_t *vaddr, vm_paddr_t end,
 		    vm_paddr_t high_water);
 vm_page_t	vm_reserv_to_superpage(vm_page_t m);
 
+/* [syncpromo] */
+bool 		vm_reserv_satisfy_sync_promotion(vm_page_t m);
+bool 		vm_reserv_satisfy_adj_promotion(vm_page_t m);
+vm_pindex_t	vm_reserv_pindex_from_page(vm_page_t m);
+void		vm_reserv_copy_popmap_from_page(vm_page_t m, u_long*);
+int 		vm_reserv_popmap_is_clear(vm_page_t m, int i);
+void		vm_reserv_mark_bad(vm_page_t m);
+bool 		vm_reserv_is_full(vm_page_t m);
+int 		vm_reserv_get_next_set_index(vm_page_t m, int i);
+int 		vm_reserv_get_next_clear_index(vm_page_t m, int i);
+
 #endif	/* VM_NRESERVLEVEL > 0 */
 #endif	/* _KERNEL */
 #endif	/* !_VM_RESERV_H_ */
